[
["index.html", "Computer vision with R and keras Chapter 1 Presentation", " Computer vision with R and keras Saif Shabou 2020-04-19 Chapter 1 Presentation "],
["foundamental-of-machine-learning.html", "Chapter 2 Foundamental of machine learning 2.1 Types 2.2 Evaluation of ML models 2.3 Overfitting and underfitting", " Chapter 2 Foundamental of machine learning https://tensorflow.rstudio.com/tutorials/advanced/customization/tensors-operations/ https://keras.rstudio.com/articles/guide_keras.html 2.1 Types Supervised learning It consists of learning to map input data to known targets, based on a set of examples. The main objectives that need supervised learning are: classification, regression, sequence generation form images, object detection. Unsupervised learning It consists of identifying interesting transformations of the input data without the use of any targets and labels. It is genrally used for: noise detection, data visualization, understanding correlation etween data features, data compression and reduction… he main objectives that need unsupervised learning are: clustering and dimensions reduction. Self-supervised learning It is a specific type of supervised learning but without huan-annotated labels. The labels used for supervising the learning process are genrated from input data itself. We can nlist: Autoencoders, temporally supervised learning (which consists of predicting the next frame in a video based the past frames). Reinforcement learning It is based on agents that receives information from the environment and learn to select actions that maximize some reward. This technique is used for learning game playing (Atari, Go…), self-driving cars… 2.2 Evaluation of ML models The main goal of machine learning models is to make them generalize and perform well on data that they have never seen. This is why we try to minimize overfitting. 2.2.1 Training, validation and test sets For evaluating models we need to split the available data into three sets: training, validation, and test. We train our model on the training set and we evaluate it on the validation set. We can modify parameters and tuning the model using these two sets (for example changing the number of layers and the hypermarameters). Once our model is ready and we identified a goog configuration, we test it on the test set. 2.2.1.1 Hold-out validation Ot consists of splitting our train data into two sets: train and validation. We train our model and evaluate it on the validation set by computing validation metrics. Hold-out validation (https://www.kdnuggets.com/2017/08/dataiku-predictive-model-holdout-cross-validation.html) # we suffle the data indices = sample(1:nrow(data), size = 0.8 *nrow(data)) # define the validation set validation_data = data[-indices, ] # define the training set training_data = data[indices, ] # train the model on the training set model %&gt;% train(training_data) # evaluate on the validation set validation_score = model %&gt;% evaluate(validation_data) # once we tuned the hyperparameters, we train our final model from scatch on all non-test data model = get_model() model %&gt;% train(data) test_score = model %&gt;% evaluate(test_data) This technique is not reommanded when we have little data available: the validation and test data contain few samples. This issue can be identified once we have different mdel performance for different shuffling round in train data splitting. In order to address this issue, we can use k-fold validation method. 2.2.1.2 k-fold validation It consists of splitting the training data into k partitions of equal size. For each partition i, the model is tained on the k-1 parititions, and evaluated on the partition i. The final score is then obtained by averaging the k scores. Cross-fold validation (https://www.kdnuggets.com/2017/08/dataiku-predictive-model-holdout-cross-validation.html) 2.3 Overfitting and underfitting 2.3.1 Reducing the network’s size The simplest way to avoid overfitting is to reduce the size of the model: the number of learnable parameters whiwh are dependant on the number of layers and the number of units by layer. 2.3.2 Adding weight regularization The weight regularizaion technics is based on the hypothesis that a simple model, with fewer parametrs or where the distribution of parameter values has less entropy, may behave better with new unseed data. Tehrefore, we can avoid overfitting by adding constraints in the model and forcing its weights to take only small values. This makes the distribution of weight vales more regular. This technic is called weight regularization. It is implemented by adding to the loss function of the netork a cost associated with having large weights. We can define the cost in two ays: L1 regularization: The cost added is propotional to the absolute value of the weight coefficients L2 regularization (weight decay): The cost added is propotional to the square of the value of the weight coefficients model = keras_model_sequential() %&gt;% layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.001), activation = &quot;relu&quot;, input_shape = c(10000)) %&gt;% layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.001), activation = &quot;relu&quot;, input_shape = c(10000)) %&gt;% layer_dense(units = 1, activation = &quot;sigmoid&quot;) regularizer_l2(0.001) means that every coefficien in the weight matrix of the layer will add 0.001*weight_coefficient_value to the total loss of the network. Since the penality is only added at training phase, the loss at the training phase will be much higher thatn the loss in the test phase. 2.3.3 Adding dropout Dropout consists on randomly setting to zero a number of output features of a layer during the training. The idea behind dropout technic is to introduce noise in the output values of a layer in order to remove non significant features. The dropout rate is the fraction of the features that are set to zero. It is usually between 0.2 and 0.5. During the test phase, no units are dopped out but the layer’s output values are scaled down by a factor equal to the dropout rate to have same number of units active. We can implement the both operations at training phase in order to have unchanged output at test phase. Therefore, we scale up by the dropout rate rather than scaling down. layer_dropout = layer_dropout * sample(0:1, length(layer_output), replace = TRUE) layer_dropout = layer_dropout / 0.5 let’s see how to add dropout in our model with keras model = keras_model_sequential() %&gt;% layer_dense(units = 16, activation = &quot;relu&quot;, input_shape = c(10000)) %&gt;% layer_dropout(rate = 0.5) %&gt;% layer_dense(units = 16, activation = &quot;relu&quot;) %&gt;% layer_dropout(rate = 0.5) %&gt;% layer_dense(units = 1, activation = &quot;sigmoid&quot;) "],
["neural-networks.html", "Chapter 3 Neural Networks 3.1 Structure of neural network 3.2 Building a Neural Network from scratch 3.3 Introduction to Keras 3.4 Classifying movie reviews: a binary classification example 3.5 The Kears Functional API 3.6 Monitoring deep learning models 3.7 Batch normalization 3.8 Hyperparameters optimization", " Chapter 3 Neural Networks 3.1 Structure of neural network Training a neural network concerns these objects: layers, input data and corresponding targets, loss function and optimizer. The network is composed of layers stacked together, maps the input data to predictions. The loss function is used to compare the predictions to the targes and generats a loss value (difference between perdicted and expected values or classes). Then, an optimizer is used used to minimize the loss value by updating the network’s weights. Neural network structure (source:https://livebook.manning.com/book/deep-learning-with-r/chapter-3/19) 3.1.1 Tensors 3.1.1.1 Dimensions The fundamental data structure manipulated by neural networks is Tensors. Tensors are a generalization of vectors and matrices to an arbitrary number of dimensions (called axis). scalars (0D tensors): A tensor with only one number is called scalar Vectors (1D tensors): A tensor with one dimension is called vector. x = c(1,2,3) str(x) ## num [1:3] 1 2 3 dim(as.array(x)) ## [1] 3 This vector has 3 possible values, so it is three-dimensional vector. This 3D vector has only one axis and 3 dimensions along its axis. In opposition to 3D tensor that has 3 axis, with any number of dimensions in each axis. Dimensionality can represent either the number of entries along a specific axis or the number of axes in a tensor. Matrices (2D tensors): A tensor with two axes (rows and columns). x &lt;- matrix(rep(0, 3*5), nrow = 3, ncol = 5) x ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 0 0 0 0 ## [3,] 0 0 0 0 0 dim(x) ## [1] 3 5 3D and higher dimensions tensors: We can obtain a 2D tensor by packing matrices in a new array. The obtained object can be represented visually as a cube. By packing the 3D tensors in an array we obtain a 4D tensor and so on. x &lt;- array(rep(0, 2*3*2), dim = c(2,3,2)) str(x) ## num [1:2, 1:3, 1:2] 0 0 0 0 0 0 0 0 0 0 ... dim(x) ## [1] 2 3 2 3.1.1.2 Properties A tensor is defined by three main prperties: Number of axis: For example, a 3D tensor has 3 axis Shape: It is an integer vector that denotes the number of dimensions the tensor has along each axis. For example, a matriw with 3 rows and 5 columns is a tensor with the shape (3,5). Data type: The type o data stored in the tensor: integer, double, character… Let’s present an example with the mnist dataset library(keras) mnist &lt;- dataset_mnist() train_images &lt;- mnist$train$x Now we display the properties the tensor train_images # number of axis length(dim(train_images)) ## [1] 3 # shape dim(train_images) ## [1] 60000 28 28 # data type typeof(train_images) ## [1] &quot;integer&quot; So we have a 3D tensor of integer. It is an array of 60,000 matrices of 28x28 integers. Let’s plot one matrix digit &lt;- train_images[1,,] plot(as.raster(digit, max = 255)) 3.1.1.3 Data batches In deep-learning models we don’t process the whole dataset at once, but we break it into small batches. It consists of slicing the samples dimension. For example we will take a batch of MNIST digits of size 128: batch &lt;- train_images[1:128,,] 3.1.1.4 Examples of data tensors Vector data They are 2D tensors of shape (samples, features) Timeseries and sequence data They are 3D tensors of shape (samples, timesteps, features) Image data They are 4D tensors of shape (samples, height, width, channels) Video data They are 5D tensors of shape (samples,frames, channels, height, width) 3.1.1.5 Tensors operations https://livebook.manning.com/book/deep-learning-with-r/chapter-2/244 3.1.2 Layers The layers cnsists on data-processing modules that take as input one or more tensors and generates as outputs one or more tensors. The layers conrain the learned weights taht represent the network’s knowledge. The layers’ types depend on the data types used as input: Densly connecte (fully connected) layers: they are used for vector data taht are stored in 2D tensors of shape (samples, features) Recurrent layers: they are used for sequence data that are stred in 3D tensors of shape samples, timsteps, features Convolution layers: they are used for ìmage data, taht are stored in 4D tensors Building deep-learning model in Keras is done by stacking compatible layers together. To respect layer compatibilty, every layer only accept input tensors with a specific shape. layer = layer_dense(units = 32, input_shape = c(784)) In this example, we are creating a layer that only accept as input 2D tensors where the first dimension is 784. This layer will return a tensor where the first dimension has been transformed to be 32. Then, this layer can only be connected to a layer taht expects as 32-dimensional vector as input. In keras, the layers we add to our models are automatically built to match the shape of the incoming mayer. model = keras_model_sequential() %&gt;% layer_dense(units = 32, input_shape = c(784)) %&gt;% layer_dense(units = 32) SO here we didn’t specify the input shape of the second layer. It was automatically inferred based on the shape of the previous layer (32). 3.1.3 Activation functions Rectified linear unit (ReLU) Sigmoid Softmax 3.1.4 Loss functions and optimizers Once the network layers architectue is defined, we have specify two elements: loss function: It represents the quantity that we want to minimize during the training phase. Optimizer: It represnts the method used by the network to update the weight based on loss function. It consists on implementing a specific variant of stochastic graient descent optimization. We can have a network with multiple loss funtions (when the network ghave multiple outpus). Since the gradient-descent process need to be based on a sigle loss value, all loasses values have to be combined i a single measure (by averaging for example). It is important to select the appropriate loss function depending in the learning problem. There are some guidlines we can follow for common situations: We use binary crossentropy for a Two-class classification problem We use categorical crossentropy for a Two-class classification problem we use the mean-squared error for a regression problem 3.2 Building a Neural Network from scratch # input matrix X=matrix(c(1,0,1,0,1,0,1,1,0,1,0,1),nrow = 3, ncol=4,byrow = TRUE) X ## [,1] [,2] [,3] [,4] ## [1,] 1 0 1 0 ## [2,] 1 0 1 1 ## [3,] 0 1 0 1 # output matrix Y=matrix(c(1,1,0),byrow=FALSE) Y ## [,1] ## [1,] 1 ## [2,] 1 ## [3,] 0 #sigmoid function sigmoid&lt;-function(x){ 1/(1+exp(-x)) } # derivative of sigmoid function derivatives_sigmoid&lt;-function(x){ x*(1-x) } # variable initialization epoch=1000 lr=0.1 inputlayer_neurons=ncol(X) hiddenlayer_neurons=3 output_neurons=1 #weight and bias initialization wh=matrix( rnorm(inputlayer_neurons*hiddenlayer_neurons,mean=0,sd=1), inputlayer_neurons, hiddenlayer_neurons) bias_in=runif(hiddenlayer_neurons) bias_in_temp=rep(bias_in, nrow(X)) bh=matrix(bias_in_temp, nrow = nrow(X), byrow = FALSE) wout=matrix( rnorm(hiddenlayer_neurons*output_neurons,mean=0,sd=1), hiddenlayer_neurons, output_neurons) bias_out=runif(output_neurons) bias_out_temp=rep(bias_out,nrow(X)) bout=matrix(bias_out_temp,nrow = nrow(X),byrow = FALSE) # forward propagation for(i in 1:epoch){ hidden_layer_input1= X%*%wh hidden_layer_input=hidden_layer_input1+bh hidden_layer_activations=sigmoid(hidden_layer_input) output_layer_input1=hidden_layer_activations%*%wout output_layer_input=output_layer_input1+bout output= sigmoid(output_layer_input) # Back Propagation E=Y-output slope_output_layer=derivatives_sigmoid(output) slope_hidden_layer=derivatives_sigmoid(hidden_layer_activations) d_output=E*slope_output_layer Error_at_hidden_layer=d_output%*%t(wout) d_hiddenlayer=Error_at_hidden_layer*slope_hidden_layer wout= wout + (t(hidden_layer_activations)%*%d_output)*lr bout= bout+rowSums(d_output)*lr wh = wh +(t(X)%*%d_hiddenlayer)*lr bh = bh + rowSums(d_hiddenlayer)*lr } output ## [,1] ## [1,] 0.93909289 ## [2,] 0.94014690 ## [3,] 0.07354235 3.3 Introduction to Keras Keras is library for developing deep learning models. It dosen’t handle low-level operations such as tensor manipulation. But it relies on specialized tensor libraries as backend engine such as TensorFlown Theano and Mirosoft Cognitive Toolkit (CNTK). Via TensorFlow, Keras is able to run on both CPUs and GPUs. When running on CPU, TensorFlow is itself wrapping a low-lvel library for tensor operations called Eigen. On GPU, TensorFlow wraps a library called NIVIDIA CUDA Neural Network library (cuDNN). 3.3.1 Installing keras # Installing keras package install.packages(&quot;keras&quot;) # Install the core keras library and TensorFlow library(keras) install_keras() This installation provide us with a default CPU-based installation of keras and TensoFlow. If we wan to train our models on a GPU and we have properly configured CUDA and cuDNN libraries, we can install the GPU-based version of TensroFlow backend engine as follows: install_keras(tensorflow = &quot;gpu&quot;) 3.3.2 building model with keras Here is a typical keras workflow: Define training data: iput tensors and target tensors Define a network of layers that maps the inputs to the targets model = keras_model_sequential() %&gt;% layer_dense(units = 32, input_shape = c(784)) %&gt;% layer_dense(units = 10, activation = &quot;softmax&quot;) Specify the learning process by defining a losss function, an optimizer, and some metrics to monitor model %&gt;% compile( optimizer = optimizer_rmsprop(lr = 0.0001), loss = &quot;mse&quot;, metrics = c(&quot;accuracy&quot;) ) Iterate on the training data using the fit() method of our model model %&gt;% fit(batch_size = 128, epochs = 10) 3.4 Classifying movie reviews: a binary classification example 3.4.1 The IMDB dataset The IMDB dataset contains a set of 50,000 reviews from the Internet Movie Databse. They are split into 25,000 reiews for training and 25,000 reviews for testing. each set contains 50% negative and 50% positive reviews. library(keras) imdb = dataset_imdb(num_words = 10000) The arguent num_words = 10000 is used to keep only the top 10,000 most frequetly occuring words in the training data. I helps for reducing the size of vector data. The imdb dataset is a nested lists of traingin and test data. train_data = imdb$train$x train_labels = imdb$train$y test_data = imdb$test$x test_labels = imdb$test$y we can extract the train and test datasets using hte multi-assignement operator %&lt;-%. c(c(train_data, train_labels), c(test_data, test_labels)) %&lt;-% imdb train_data and test_data are lists of reviews. Each review is a list of word indices (encoding a sequence of words. train_labels and test_labels are lists of 0 (negative review) ad 1 (positive reviews). str(train_data[[1]]) train_labels[[1]] 3.4.2 preparing the data We can’t feed lists of integers into a neural network. We have to transform them into tensors. We can one-hot encode our lists to tranform them into vectors of 0s and 1s. For example, we can transform the sequence [3,5] into a 10,000 dimensional vector with 0 everywhere except for indices 3 and 5 that contain 1s. Let’s make a function to vectorize our data vectorize_sequences = function(sequences, dimension = 10000){ # create an all-zero matrix results = matrix(0, nrow = length(sequences), ncol = dimension) for(i in 1:length(sequences)) results[i, sequences[[i]]] = 1 # sets specific indices of results[i] to 1s results } x_train = vectorize_sequences(train_data) x_test = vectorize_sequences(test_data) # show a sample str(x_train[1, ]) We have to convert our labels from integer to numeric y_train = as.numeric(train_labels) y_test = as.numeric(test_labels) 3.4.3 Build the network Since the input data have vector types and the labels are scalars (1 or 0), we can use fully connected (dense) layers with relu activations: layer_dense(units=16, activation = \"relu\" 3.5 The Kears Functional API The common configuration of keras models is based on sequential layer stacking keras_model_sequential. However, in some applications we may need to develop models with multiple inputs, multiple outputs and more complex interactions between layers. These make these model look lik graphs. Example of multi-inputs: let’s imagine we are facing to a learning problem based on text ad image data (for example predicting item prices based on text description and pictures). We can propose two seperate models and average the prediction outputs at the end. Another way consists on jointly learn a more accurate model based on both text and image inputs. Example of multi-outpus: let’s imagine we have some text corpus and we want to predict both the type (romance vs thriller classification) and the date when it was written. We can train two sperate models: a classifier for text types and a regressor for date prediction. But since these properties may be dependents and correlated, it would be more accurate to build a odel that jointly learn to predict types and date. Example of complex architectures: Many recent networks architecture require complex connexions between layers or group of layers: The Inception family of CNN process several parallel convolutional branches whose outputs are merged back into a single tensor, the ResNet family add residual connections that consists of reinjecting past output tensor to later output tensor in order to prevent inofmation loss along the data processing flow. Inception module (source: https://www.oreilly.com/library/view/hands-on-convolutional-neural/9781789130331/41617ecf-cd1e-467c-93d9-ecf265979317.xhtml) Residual connections (source: https://towardsdatascience.com/introduction-to-resnets-c0a830a288a4) Here is how to define an equivalent to sequential model but using the functional API in a graph like model. # sequential model seq_model = keras_model_sequential() %&gt;% layer_dense(units = 72, activation = &quot;relu&quot;, input_shape = (64)) %&gt;% layer_dense(units = 32, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 10, activation = &quot;softmax&quot;) # equivalent functional model input_tensor = layer_input(shape = c(64)) output_tensor = input_tensor %&gt;% layer_dense(units = 32, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 32, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 10, activation = &quot;softmax&quot;) model = keras_model(input_tensor,output_tensor) model 3.5.1 Multi-input models 3.5.2 Multi-output models 3.5.3 Directed acuclic graphs of layers With the functional API, we can build models with complex architecture. Keras provides the possibility of modeling layers as direcetd acyclic graphs. It can be used for building models with inception like topology branch_a = input %&gt;% layer_conv2d(filters = 128, kernel_size = 1, activation = &quot;relu&quot;, strides = 2) branch_b = input %&gt;% layer_conv2d(filters = 128, kernel_size = 1, activation = &quot;relu&quot;) %&gt;% layer_conv2d(filters = 128, kernel_size = 3, activation = &quot;relu&quot;, strides = 2) branch_c = input %&gt;% layer_average_pooling_2d(pool_size = 3, strides = 2) %&gt;% layer_conv2d(filters = 128, kernel_size = 3, activation = &quot;relu&quot;) branch_d = input %&gt;% layer_conv2d(filters = 128, kernel_size = 1, activation = &quot;relu&quot;) %&gt;% layer_conv2d(filters = 128, kernel_size = 3, activation = &quot;relu&quot;) %&gt;% layer_conv2d(filters = 128, kernel_size = 3, activation = &quot;relu&quot;, strides = 2) output = layer_concatenate(list(branch_a, branch_b, branch_c, branch_d)) 3.6 Monitoring deep learning models 3.6.1 Using callbacks A callback is an object integrated to the model in the fit operation and is caled at different points durinf training phase. The callback object has access toall information about the state of the model and its performance. It can also act during the training for: Model checkpointing: saving the current weights of the mode at different points during the training Early stopping: Interrupting training when the validation loss is no longer improving and saving the best model obtained during training Dynamically adjusting the value of certain parameters during training such as the learning rate and the optimizer Logging training and validation metrics Keras provides some built-in callbacks: callback_model_checkpoint() callback_early_stopping() callback_learning_rate_scheduler() callback_reduce_lr_on_plateau() callback_csv_logger() We can use callback_early_stopping to interrupt training once a target monitored metric gas stopped improving during some epochs. This type of callback can be used to stop training when the model starts overfitting. This callback is used in simultaneously with callback_model_checkpoint that enables saving continually the model during the training phase. We can also choose to save the optimal version of the model: the version that ends with best performance at the end of an epoch. callbacks_list = list( # stop training when no improvement callback_early_stopping( # monitor accuracy metric monitor = &quot;acc&quot;, # Stop training when accuracy has stopped improving for more than one epoch patience = 1 ), # saving the current weights after every epoch callback_model_checkpoint( filepath = &quot;my_model.h5&quot;, # we avoide overwriting the model file unless val_loss has improved. We keen the best model seen during training. monitor = &quot;val_loss&quot;, save_best_only = TRUE ) ) model %&gt;% compile( optimizer = &quot;rmsprop&quot;, loss = &quot;binary_crossentropy&quot;, metrics = c(&quot;acc&quot;) ) model %&gt;% fit( x, y, epochs = 10, batch_size = 32, callbacks = callbacks_list, # because the callback ill monitor the validation loss, we ned to pass validation_data to the call to fit validation_data = list(x_val, y_val) ) The ‘callbacks’ can be used for reducing the learning rate once validation loss has stopped improving. reducing or increasing the learning rate in case of loss plateau is consiedered as an effective strategy to get out of local minima dring training. callbacks_list = list( callback_reduce_lr_on_plateau( # monitor the model validation loss monitor = &quot;val_loss&quot;, # dividing the learning rate by 10 when triggered factor = 0.1, # the callback is triggered after the validation loss has stopped improving for 10 epochs patience = 10 ) ) model %&gt;% fit( x, y, epochs = 10, batch_size = 32, callbacks = callbacks_list, # because the callback ill monitor the validation loss, we ned to pass validation_data to the call to fit validation_data = list(x_val, y_val) ) 3.6.2 TensorBoard TensorBoard is a browser-based visualization tool packaged with TensorFlow. It helps to visually montor mterics during training, show model architecture, visalize histograms and activations and gradients… Here is an example of using TensorBoard fo IMDB sentiment-analysis application. # limiing the number of words to consider as features for this example max_features = 2000 # cut text in 500 words for this example max_len = 500 # load data imdb = dataset_imdb(num_words = max_features) # split train and test c(c(x_train, y_train),c(x_test, y_test)) %&lt;-% imdb # pad sequences x_train = pad_sequences(x_train, maxlen = max_len) x_test = pad_sequences(x_test, maxlen = max_len) # build model model = keras_model_sequential() %&gt;% layer_embedding(input_dim = max_features, output_dim = 128, input_length = max_len, name = &quot;embed&quot;) %&gt;% layer_conv_1d(filters = 32, kernel_size = 7, activation = &quot;relu&quot;) %&gt;% layer_max_pooling_1d(pool_size = 5) %&gt;% layer_conv_1d(filters = 32, kernel_size = 7, activation = &quot;relu&quot;) %&gt;% layer_global_max_pooling_1d() %&gt;% layer_dense(units = 1) summary(model) # compile model model %&gt;% compile( optimizer = &quot;rmsprop&quot;, loss = &quot;binary_crossentropy&quot;, metrics = c(&quot;acc&quot;) ) We create a directory where we can store the log files generated dir.create(&quot;log_directory&quot;) # lunch tensorboard tensorboard(&quot;log_directory&quot;) # define callbacks callbacks = list( callback_tensorboard( log_dir = &quot;log_directory&quot;, # record activation histograms every 1 epoch histogram_freq = 1, # record embedding data every 1 epoch embeddings_freq = 1, ) ) # fit the model history = model %&gt;% fit( x_train, y_train, epochs = 5, batch_size = 128, validation_split = 0.2, calbacks = callbacks ) 3.7 Batch normalization Batch normalization helps mdels learn and generalize better on new data. The most common methods for batch normalization consists of centering the data on zero by substracting the mean from the data and changing standard deviation of data to 1 by dif=viding the data by ots standard deviation. mean = apply(train_data, 2, mean) std = apply(train_data, 2, sd) train_data = scale(train_data, center = mean, scale = std) test_data = scale(test_data, center = mean, scale = std) Batch normalization should be implemented after every transofmation operated by the network. In keras, it is represenred as a specific layer: layer_batch_normalization. It is commonly used after a convolution or densly connected layers: layer_conv_2d(filters = 32, kernel_size = 3, activation = &quot;relu&quot;) %&gt;% layer_batch_normalization() layer_dense(units = 32, activation = &quot;relu&quot;) %&gt;% layer_batch_normalization() 3.8 Hyperparameters optimization https://tensorflow.rstudio.com/tools/tfruns/overview/ "],
["deep-learning-for-computer-vision.html", "Chapter 4 Deep learning for computer vision 4.1 Loading image data 4.2 Image classification with Keras 4.3 Introduction to COnvolution Neural Networks 4.4 Architectures of CNN 4.5 Application Dog VS Cats 4.6 CNN with CIFAR10 dataset 4.7 CNN with fruits images", " Chapter 4 Deep learning for computer vision 4.1 Loading image data library(keras) library(tfdatasets) # downloading the data in a local directory data_dir = get_file( origin = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;, fname = &quot;D:/image/DeepLearning-ComputerVision/data/flower_photos.tgz&quot;, extract = TRUE ) data_dir = file.path(dirname(data_dir), &quot;flower_photos&quot;) 4.2 Image classification with Keras We will see here a simple image classification example using Keras based on the MNIST dataset. 4.2.1 download and prepare the data library(keras) # load data mnist = dataset_mnist() # rescale the data mnist$train$x = mnist$train$x / 255 mnist$test$x = mnist$test$x / 255 # dimensions dim(mnist$train[[1]]) dim(mnist$test[[1]]) 4.2.2 Build the model model = keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28,28)) %&gt;% # we have to specify the input dimensions for the first layer. In our case we have imahes of 28x28 layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) 4.2.3 Compile the model model %&gt;% compile( loss = &quot;sparse_categorical_crossentropy&quot;, optimizer = &quot;adam&quot;, metrics = &quot;accuracy&quot; ) 4.2.4 Fit the model model %&gt;% fit( x = mnist$train$x, y = mnist$train$y, epochs = 5, # iterations validation_split = 0.3, # fraction of the training data to be used as validation data verbose = 2 # 0= silent, 1=progress bar,2=one line per epoch ) 4.2.5 Make predictions predictions &lt;- predict(model, mnist$test$x) head(predictions, 2) predictions &lt;- predict_classes(model, mnist$test$x) head(predictions, 2) predictions &lt;- predict_proba(model, mnist$test$x) head(predictions, 2) 4.2.6 Evaluate the model model %&gt;% evaluate(mnist$test$x, mnist$test$y, verbose = 0) 4.2.7 Save the model save_model_tf(object = model, filepath = &quot;D:/image/DeepLearning-ComputerVision/models/mnist&quot;) save_model_hdf5(object = model, filepath = &quot;D:/image/DeepLearning-ComputerVision/models/Mnist_hdf5&quot;) 4.2.8 reload the model reloaded_model &lt;- load_model_tf(&quot;D:/image/DeepLearning-ComputerVision/models/mnist&quot;) reloaded_model_hdf5 = load_model_hdf5(&quot;D:/image/DeepLearning-ComputerVision/models/Mnist_hdf5&quot;) predictions_reloaded = predict(reloaded_model_hdf5, mnist$test$x) head(predictions_reloaded) head(predictions) 4.3 Introduction to COnvolution Neural Networks 4.3.1 Example The following lines of code show a basic convnet model structure. It is a stack of layer_conv_2d and layer_max_pooling_2d layers. library(keras) model = keras_model_sequential() %&gt;% layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = &quot;relu&quot;, input_shape = c(28,28,1)) %&gt;% layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = &quot;relu&quot;) A convnet takes as input tensors of shape (image_height, image_width, image_channels). In the example above, we configured the convnet to process inputs of size (28, 28, 1) by specifying the argument input_shape = c(28,28,1). Let’s show the odel architecture model Then we feed the last output tensor (of shape (3,3,64)) into a densly connected classifier network. Since the classifiers process vectors (1D), we need to flatten the 3D outputs to 1D before adding dense layers on top. model = model %&gt;% layer_flatten() %&gt;% layer_dense(units = 64, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 10, activation = &quot;softmax&quot;) model We see that the (3,3,64) ouputs are flattened into vectors of shape (576) before being feeded to dense layers. Now let’s train the convnet on the MNIST digits data. mnist = dataset_mnist() c(c(train_images, train_labels), c(test_images, test_labels)) %&lt;-% mnist train_images = array_reshape(train_images, c(60000, 28, 28, 1)) train_images = train_images / 255 test_images = array_reshape(test_images, c(10000, 28, 28, 1)) test_images = test_images / 255 train_labels = to_categorical(train_labels) test_labels = to_categorical(test_labels) model %&gt;% compile( optimizer = &quot;rmsprop&quot;, loss = &quot;categorical_crossentropy&quot;, metrics = c(&quot;accuracy&quot;) ) model %&gt;% fit( train_images, train_labels, epochs = 5, batch_size = 64 ) let’s evaluate the model on the test data results = model %&gt;% evaluate(test_images, test_labels) results 4.3.2 The convolution operation The objective of convolution layers is to learn local patterns. They have two main charecteristics: They are translation invariant They can learn spatial hierarchies of pattterns In te MNIST example, the first convolution layer takes a feature map of size (28,28,1) and outputs a feature map of size (26,26,32): it computes 32 filters over its inputs. Each output contains a 26x26 grid of values representing a response map of the filter over different locations of the input. Convolutions are defined based on two main parameters: Size of patches extracted from the inputs: We often use 3 x 3 or 5 x 5 patches. Depth of the output feature map: It represents the number of filters computes by the convolution. In the example, we started with a depth of 32 and ended with a depth of 64. The convolutio works by sliding windows of size 3x3 or 5x5 over the input feature map and extracting at every location the patch of features. Each patch is then transformed in a 1D vector by computing a tensor product with the weight matrix called convolution kernel. We remark that the output width and height can be different from the input widthand height because of border effects and used strides. 4.3.2.1 Border effects and padding Border effects make that the output size feature map of convolution is less large that the input feature map (in the previous example from 28x28 to 26x26). We can avoid this effect by using padding, which consists on adding an appropriate number of rows and columns on each side of the input feature map to make it possible to fit center convolution windows around every input tile. 4.3.2.2 Convolution strides The stride represents the distance between two successive windows. 4.3.3 The max-pooling operation Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel. It is usually done with 2x2 windows and stride of 2, in order to downsample the feature maps by a factor of 2. This operation helps in reducing he number of feature-map coeficients to process. 4.4 Architectures of CNN LeNet-5 (1998) It has 2 convolutional with average pooling and 3 fully-connected layers. The activation function is Tanh. LeNet-5 (https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d) AlexNet (2012) AlexNet has 8 layers: 5 convolutional with maxpooling and 3 fully connected. The activation function is ReLU. AlexNet (https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d) VGG-16 (2014) It has 13 convolutional and 3 fully connected layers. VGG-16 (https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d) Inception v1 - GoogleNet (2014) It is a Network in Network approach. Inception v1 (https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d) ResNet-50 (2015) ResNet-50 (https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d) Xception (2016) Xception (https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d) DenseNet MobileNet 4.5 Application Dog VS Cats 4.5.1 Downloading data We will use the cats vs dogs dataset from kaggle. It contains 25,000 images of dogs and cates (12,500 for each class). After downloading the data we will create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class. original_dataset_dir = &quot;D:/image/DeepLearning-ComputerVision/data/dogs-vs-cats/train/train&quot; # ----------------------- Create base directories # base_dir base_dir = &quot;D:/image/DeepLearning-ComputerVision/data/cats_and_dogs_small&quot; dir.create(base_dir) # train_dir train_dir = file.path(base_dir, &quot;train&quot;) dir.create(train_dir) # validation_dir validation_dir = file.path(base_dir, &quot;validation&quot;) dir.create(validation_dir) # test_dir test_dir = file.path(base_dir, &quot;test&quot;) dir.create(test_dir) # ----------------------- train directories # train_cats_dir train_cats_dir = file.path(train_dir, &quot;cats&quot;) dir.create(train_cats_dir) # train_dogs_dir train_dogs_dir = file.path(train_dir, &quot;dogs&quot;) dir.create(train_dogs_dir) # ----------------------- validation directories # validation_cats_dir validation_cats_dir = file.path(validation_dir, &quot;cats&quot;) dir.create(validation_cats_dir) # validation_dogs_dir validation_dogs_dir = file.path(validation_dir, &quot;dogs&quot;) dir.create(validation_dogs_dir) # ----------------------- test directories # test_cats_dir test_cats_dir = file.path(test_dir, &quot;cats&quot;) dir.create(test_cats_dir) # test_dogs_dir test_dogs_dir = file.path(test_dir, &quot;dogs&quot;) dir.create(test_dogs_dir) # ----------------------- copy and rename files # ---------- cats # train_cats_dir fnames = paste0(&quot;cat.&quot;,1:1000, &quot;.jpg&quot;) file.copy(file.path(original_dataset_dir, fnames), file.path(train_cats_dir)) # validation_cats_dir fnames = paste0(&quot;cat.&quot;,1001:1500, &quot;.jpg&quot;) file.copy(file.path(original_dataset_dir, fnames), file.path(validation_cats_dir)) # test_cats_dir fnames = paste0(&quot;cat.&quot;,1501:2000, &quot;.jpg&quot;) file.copy(file.path(original_dataset_dir, fnames), file.path(test_cats_dir)) # ---------- dogs # train_dogs_dir fnames = paste0(&quot;dog.&quot;,1:1000, &quot;.jpg&quot;) file.copy(file.path(original_dataset_dir, fnames), file.path(train_dogs_dir)) # validation_dogs_dir fnames = paste0(&quot;dog.&quot;,1001:1500, &quot;.jpg&quot;) file.copy(file.path(original_dataset_dir, fnames), file.path(validation_dogs_dir)) # test_dogs_dir fnames = paste0(&quot;dog.&quot;,1501:2000, &quot;.jpg&quot;) file.copy(file.path(original_dataset_dir, fnames), file.path(test_dogs_dir)) # check cat(&quot;total training cat images:&quot;, length(list.files(train_cats_dir)), &quot;\\n&quot;) cat(&quot;total training dog images:&quot;, length(list.files(train_dogs_dir)), &quot;\\n&quot;) cat(&quot;total validation cat images:&quot;, length(list.files(validation_cats_dir)), &quot;\\n&quot;) cat(&quot;total validation dog images:&quot;, length(list.files(validation_dogs_dir)), &quot;\\n&quot;) cat(&quot;total test cat images:&quot;, length(list.files(test_cats_dir)), &quot;\\n&quot;) cat(&quot;total test dog images:&quot;, length(list.files(test_dogs_dir)), &quot;\\n&quot;) 4.5.2 Building the network library(keras) model = keras_model_sequential() %&gt;% layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = &quot;relu&quot;, input_shape = c(150,150,3)) %&gt;% layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% layer_flatten() %&gt;% layer_dense(units = 512, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 1, activation = &quot;sigmoid&quot;) summary(model) For the compilation, we will use the RMSprop optimizer. Because the network ended with a sngle sigmoid unit, we will use binary crossentropy as the loss. model %&gt;% compile( loss = &quot;binary_crossentropy&quot;, optimizer = optimizer_rmsprop(lr = 0.001), metrics = c(&quot;acc&quot;) ) 4.5.3 Data preprocessing We need to preprocess the image data before feeding the network: decode the JPEG content to RGB grids of pixels, convert them into floating-point tensors, and rescaling the pixel values from [0,255] to [0,1] interval. Keras provides some image processing tools like image_data_generator function that turn auomatically image files on disk into batches of preprocessed tnesors. # rescale all images bu 1/255 train_datagen = image_data_generator(rescale = 1/255) validation_datagen = image_data_generator(rescale = 1/255) train_generator = flow_images_from_directory( train_dir, # Traget directory train_datagen, # training data generator target_size = c(150, 150), # resize all images to 150 x 150 batch_size = 20, class_mode = &quot;binary&quot; # because we use binary_crossentropy loss ) validation_generator = flow_images_from_directory( validation_dir, validation_datagen, target_size = c(150, 150), batch_size = 20, class_mode = &quot;binary&quot; ) The output of these generators constists on batches of 150x150 RGB iages and binary labels. Each batch contains 20 samples (batch size). batch = generator_next(train_generator) str(batch) We use the fit_generator function to fit the model using the generator. The fitting process needs to know how many samples to draw from the genrator before declaring an epoch over. In this example, we have batches of 20 amples, so we need 100 batches to process the whole 2000 samples. We need to specify the same thing for the validation data. Since we have 1000 validation samples, we need 50 validation steps of batches of 20 images. history = model %&gt;% fit_generator( train_generator, steps_per_epoch = 100, epochs = 5, validation_data = validation_generator, validation_steps = 50 ) We can save our model model %&gt;% save_model_hdf5(&quot;D:/image/DeepLearning-ComputerVision/models/cats_and_dogs_small_1.h5&quot;) We can plot the loss and accuracy of the model over the training and validation data during training. It shows an overfitting phenomenon. The training accuracy increases linearly over time wheras tha validation accuracy stops under a less important vale. plot(history) 4.5.4 Data augmentation Iverfitting can be caused by the small quantity of samples for learning. Data augmentation onsists of generating more training data from existing training samples,by augmenting the samples via a number of random transformations that generates realistic and possible images. This helps in exposing the model to more aspects of the data and generalize better. In keras, we candefine a number of random trasformations on the images with image_data_generator datagen = image_data_generator( rescale = 1/255, rotation_range = 40, # randomly rotate the pictures width_shift_range = 0.2, # a fraction of total width within which translate pictires horizontally height_shift_range = 0.2, # a fraction of total height within which translate pictires vertically shear_range = 0.2, # shifting image zoom_range = 0.2, # zooming inside the picture horizontal_flip = TRUE, fill_mode = &quot;nearest&quot; ) We can plot transformation effects one sample of image fnames = list.files(train_cats_dir, full.names = TRUE) img_path = fnames[[2]] # choose one image to argument img = image_load(img_path, target_size = c(150,150)) # read the image and resize it img_array = image_to_array(img) # convert the image to an array of shape (150,150,3) img_array = array_reshape(img_array, c(1,150,150,3)) # reshape the array # generate batches of randomly transformed images augmentation_generator = flow_images_from_data( img_array, generator = datagen, batch_size = 1 ) # plot the images op = par(mfrow = c(2,2), pty = &quot;s&quot;, mar = c(1,0,1,0)) for ( i in 1:4){ batch = generator_next(augmentation_generator) plot(as.raster(batch[1,,,])) } par(op) Now wa can train the network using data augmentation generator # apply data augmentation generator to train train_generator = flow_images_from_directory( train_dir,#traget directory datagen,# data generator target_size = c(150,150), batch_size = 32, class_mode = &quot;binary&quot; ) # load test data test_datagen = image_data_generator(resscale = 1/255) # the validation data shouldn&#39;t be augmented validation_generator = flow_images_from_directory( validation_dir, test_datagen, target_size = c(150,150), batch_size = 32, class_mode = &quot;binary&quot; ) # fit the model history = model %&gt;% fit_generator( train_generator, steps_per_epoch = 100, epochs = 5, validation_data = validation_generator, validation_steps = 50 ) let’s save the model model %&gt;% save_model_hdf5(&quot;D:/image/DeepLearning-ComputerVision/models/cats_and_dogs_small_2.h5&quot;) plot(history) 4.6 CNN with CIFAR10 dataset 4.6.1 Download and prepare the CIFAR10 dataset We will use CNN to classifiy CIFAR10 dataset which consists of 60,000 color images in 10 classes (6,000 images in each class). It contains 50,000 images for training and 10,000 images for testing. library(tensorflow) library(keras) cifar = dataset_cifar10() dim(cifar$train$x) We can plot the first 25 images to verify the data: cifar = dataset_cifar10() class_names &lt;- c(&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;) index &lt;- 1:30 par(mfcol = c(5,6), mar = rep(1, 4), oma = rep(0.2, 4)) cifar$train$x[index,,,] %&gt;% purrr::array_tree(1) %&gt;% purrr::set_names(class_names[cifar$train$y[index] + 1]) %&gt;% purrr::map(as.raster, max = 255) %&gt;% purrr::iwalk(~{plot(.x); title(.y)}) 4.6.2 Build the model We will build a convolution base for the model follwing a common pattern: a stck of Conv2D and MaxPooling2D layers. As input, a CNN takes tensors of shape (image_height, image_width, channels). The format of cifar images is (32,32,3). model = keras_model_sequential() %&gt;% layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = &quot;relu&quot;, input_shape = c(32,32,3)) %&gt;% layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = &quot;relu&quot;) summary(model) We will add a dense layr on the top of our covolutional base. Dense layers take vectors as input (which are 1D). So, we need to flatten the 3D output of the convoltion base to 1D before adding layers on top. CIFAR data has 10 output classes, so the last layer need 10 outputs and a softmax activation. model %&gt;% layer_flatten() %&gt;% layer_dense(units = 64, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 10, activation = &quot;softmax&quot;) summary(model) 4.6.3 Compile and train the model # launch TensorBoard (data won&#39;t show up until after the first epoch) # tensorboard(&quot;logs/run_a&quot;) model %&gt;% compile( optimizer = &quot;adam&quot;, loss = &quot;sparse_categorical_crossentropy&quot;, metrics = &quot;accuracy&quot; ) history &lt;- model %&gt;% fit( x = cifar$train$x, y = cifar$train$y, epochs = 5, validation_data = unname(cifar$test), verbose = 2 # callbacks = callback_tensorboard(&quot;logs/run_a&quot;) ) 4.6.4 Evaluate the model plot(history) evaluate(model, cifar$test$x, cifar$test$y, verbose = 0) 4.7 CNN with fruits images "],
["transfer-learning.html", "Chapter 5 Transfer learning 5.1 Knowledge 5.2 Transfer learning with TensforFlow Hub 5.3 Transfer learningusing a pretrained CONVNET", " Chapter 5 Transfer learning https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751 https://keras.io/applications/#available-models 5.1 Knowledge 5.1.1 What is Transfer Learning mobilenet 5.1.2 What is TensforFlow Hub 5.2 Transfer learning with TensforFlow Hub We load necessary packages library(keras) library(tfhub) library(pins) 5.2.1 ImageNet classifier 5.2.1.1 Download the classifier we can use layer_hub to load a mobilenet and wrap it as a keras layer. We have to select one classifier URL from (tfhub.dev)[https://tfhub.dev/]. # select the classifier URL classifier_url =&quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2&quot; # define the image shape image_shape &lt;- c(224L, 224L, 3L) # load the classifier classifier &lt;- layer_hub(handle = classifier_url, input_shape = image_shape) 5.2.1.2 Run the classifier on one image In order to verify the classifier, we can run it on a single image # we download an image image_url &lt;- &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg&quot; img &lt;- pins::pin(image_url, name = &quot;grace_hopper&quot;) %&gt;% tensorflow::tf$io$read_file() %&gt;% tensorflow::tf$image$decode_image(dtype = tf$float32) %&gt;% tensorflow::tf$image$resize(size = image_shape[-3]) we can plot the image img %&gt;% as.array() %&gt;% as.raster() %&gt;% plot() We add a batch dimension and pass the image to the model. result &lt;- img %&gt;% tf$expand_dims(0L) %&gt;% classifier() print(result) The result consists on a 1001 element vector of logits, rating the probability of each class for the ilages. We use argmax in order to find the top class ID. predicted_class &lt;- tf$argmax(result, axis = 1L) %&gt;% as.integer() predicted_class 5.2.1.3 Decode the prediction The classifier predicted that our image belong to the class of ID: 653. We need to identify the label corresponding to this class in the ImageNet data. # we download the labels labels_url &lt;- &quot;https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt&quot; imagenet_labels &lt;- pins::pin(labels_url, &quot;imagenet_labels&quot;) %&gt;% readLines() We will plot the image with the label corresponding to teh predicted class img %&gt;% as.array() %&gt;% as.raster() %&gt;% plot() # title(paste(&quot;Prediction:&quot; , imagenet_labels[predicted_class + 1])) 5.2.2 Transfer learning With the use of TF Hub we can retrain the top layer of the model to recognize the classes in our dataset flowers &lt;- pins::pin(&quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;, &quot;flower_photos&quot;) We can use image_data_generator to load this data into our model. Since TensorFlow Hub’s image modules used float inputs that range between 0 and 1, we have to rescale our images using image_data_generator image_generator &lt;- image_data_generator(rescale=1/255) image_data &lt;- flowers[1] %&gt;% dirname() %&gt;% dirname() %&gt;% flow_images_from_directory(image_generator, target_size = image_shape[-3]) The reulting object is an iterator that returns image_batch, label_batch pairs. We can iterate over it using the iter_nex function. str(reticulate::iter_next(image_data)) 5.2.3 Run the classifier on a batch of images image_batch &lt;- reticulate::iter_next(image_data) predictions &lt;- classifier(tf$constant(image_batch[[1]], tf$float32)) predicted_classnames &lt;- imagenet_labels[as.integer(tf$argmax(predictions, axis = 1L) + 1L)] We can plot the predicted classes with the images in order to evaluate the classifier performance par(mfcol = c(4,8), mar = rep(1, 4), oma = rep(0.2, 4)) image_batch[[1]] %&gt;% purrr::array_tree(1) %&gt;% purrr::set_names(predicted_classnames) %&gt;% purrr::map(as.raster) %&gt;% purrr::iwalk(~{plot(.x); title(.y)}) 5.2.4 Download a headless model In TensorFlow Hub we can use models without the top classification layer. feature_extractor_url &lt;- &quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2&quot; We create a festure extractor feature_extractor_layer &lt;- layer_hub(handle = feature_extractor_url, input_shape = image_shape) It returns a 1280 length vector for each image feature_batch &lt;- feature_extractor_layer(tf$constant(image_batch[[1]], tf$float32)) feature_batch Freeze the variables in the feature extractor layer, so that the training only modifies the new classifier layer. freeze_weights(feature_extractor_layer) 5.2.5 Attach a classification head Now let’s create a sequential model using the feature extraction layer and add a new classification layer. model &lt;- keras_model_sequential(list( feature_extractor_layer, layer_dense(units = image_data$num_classes, activation=&#39;softmax&#39;) )) summary(model) 5.2.6 Train the model Use compile to configure the training process: model %&gt;% compile( optimizer = &quot;adam&quot;, loss = &quot;categorical_crossentropy&quot;, metrics = &quot;accuracy&quot; ) Now we use the fit method to train the model history &lt;- model %&gt;% fit_generator( image_data, epochs=2, steps_per_epoch = image_data$n / image_data$batch_size, verbose = 2 ) Now just after 2 iterations we can see that the model is maing progress in the classification performance. 5.2.7 Export the model Now we can save our trained model save_model_tf(model, &quot;mymodel/&quot;, include_optimizer = FALSE) Now confirm that we can reload it, and it still gives the same results: model_ &lt;- load_model_tf(&quot;mymodel/&quot;) x &lt;- tf$constant(image_batch[[1]], tf$float32) all.equal( as.matrix(model(x)), as.matrix(model_(x)) ) 5.3 Transfer learningusing a pretrained CONVNET 5.3.1 feature extraction Feature extraction consits of using the representations learned by a trained network to extract interesting features from new samples. These features are then used in anew classifier trained from scratch. For convnets models, fature extraction consists of taking the convolution base of a previously trained network, running the new data through it, and training a new classifier on top of the output. We reuse the convolution base because it is likely to be more generic compared to the representations learned by the classifier that are largely depending on the set of classes used for training. The level og generality of the representations extracted by specific convolution layers depends on the depth of the layer in the model. Layers that come earlier in the model extract local, highly generic feature maps (such as visual edges, colors, and textures), wheras layers that are higher up extract more abstract concepts (such as “cat ear” or “dog eye”). So our new dataset differs a lot from the dataset on which the original model was trained, it would be better to use only the first new layers of the model for feature extraction instead of using the hwole convolutional base. In this example, we will use the VGG16 network, trained on ImageNet data. There are sevral image-classificationtrained models on Imagenet data: Xception, Inception V3, ResNet50, VGG16, VGG19, MobileNet… conv_base = application_vgg16( weights = &quot;imagenet&quot;, include_top = FALSE, #including or not the densly connected classifier on top of the network. input_shape = c(150,150,3) ) conv_base There are two main methods to use to convolutional base network in our data: Running the convolutional base over our dataset, recording its output to an array on disk, and then using this data as input to a densly connected classifier. This method is fast and cheap to run, because it only requires running the convolutional base once for evey input image which is not the most expensive part of our model. Adding dense layers on top of the convolutional base and running the whole model on the input data. This solution is expensive to run since we need to ru the whole model. 5.3.1.1 Fast feature extraction without data augmentation 5.3.1.2 feature extraction without data augmentation model = keras_model_sequential() %&gt;% conv_base %&gt;% layer_flatten() %&gt;% layer_dense(units = 256, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 1, activation = &quot;sigmoid&quot;) Before compiling the model, we need to freeze the convolutional base: preventing the layers’ weights from being updated during the training. freeze_weights(conv_base) 5.3.2 Fine-tuning "],
["visualizing-what-convnets-leran.html", "Chapter 6 Visualizing what CONVNETS leran 6.1 Visualizing intermediate activations 6.2 Visualizing convnet filters 6.3 Visualizing heatmaps of class activation", " Chapter 6 Visualizing what CONVNETS leran 6.1 Visualizing intermediate activations 6.2 Visualizing convnet filters 6.3 Visualizing heatmaps of class activation "],
["references.html", "References", " References "]
]
