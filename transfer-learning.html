<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Transfer learning | Computer vision with R and keras</title>
  <meta name="description" content="This book conains tutorials for deep learning applications in computer vision" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Transfer learning | Computer vision with R and keras" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book conains tutorials for deep learning applications in computer vision" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Transfer learning | Computer vision with R and keras" />
  
  <meta name="twitter:description" content="This book conains tutorials for deep learning applications in computer vision" />
  

<meta name="author" content="Saif Shabou" />


<meta name="date" content="2020-04-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="deep-learning-for-computer-vision.html"/>
<link rel="next" href="visualizing-what-convnets-leran.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computer vision with Keras and R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Presentation</a></li>
<li class="chapter" data-level="2" data-path="foundamental-of-machine-learning.html"><a href="foundamental-of-machine-learning.html"><i class="fa fa-check"></i><b>2</b> Foundamental of machine learning</a><ul>
<li class="chapter" data-level="2.1" data-path="foundamental-of-machine-learning.html"><a href="foundamental-of-machine-learning.html#types"><i class="fa fa-check"></i><b>2.1</b> Types</a></li>
<li class="chapter" data-level="2.2" data-path="foundamental-of-machine-learning.html"><a href="foundamental-of-machine-learning.html#model-performance-evaluation"><i class="fa fa-check"></i><b>2.2</b> Model performance evaluation</a><ul>
<li class="chapter" data-level="2.2.1" data-path="foundamental-of-machine-learning.html"><a href="foundamental-of-machine-learning.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>2.2.1</b> Training, validation and test sets</a></li>
<li class="chapter" data-level="2.2.2" data-path="foundamental-of-machine-learning.html"><a href="foundamental-of-machine-learning.html#evaluation-metrics"><i class="fa fa-check"></i><b>2.2.2</b> Evaluation metrics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>3</b> Neural Networks</a><ul>
<li class="chapter" data-level="3.1" data-path="neural-networks.html"><a href="neural-networks.html#structure-of-neural-network"><i class="fa fa-check"></i><b>3.1</b> Structure of neural network</a><ul>
<li class="chapter" data-level="3.1.1" data-path="neural-networks.html"><a href="neural-networks.html#tensors"><i class="fa fa-check"></i><b>3.1.1</b> Tensors</a></li>
<li class="chapter" data-level="3.1.2" data-path="neural-networks.html"><a href="neural-networks.html#layers"><i class="fa fa-check"></i><b>3.1.2</b> Layers</a></li>
<li class="chapter" data-level="3.1.3" data-path="neural-networks.html"><a href="neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>3.1.3</b> Activation functions</a></li>
<li class="chapter" data-level="3.1.4" data-path="neural-networks.html"><a href="neural-networks.html#loss-functions-and-optimizers"><i class="fa fa-check"></i><b>3.1.4</b> Loss functions and optimizers</a></li>
<li class="chapter" data-level="3.1.5" data-path="neural-networks.html"><a href="neural-networks.html#building-a-neural-network-from-scratch"><i class="fa fa-check"></i><b>3.1.5</b> Building a Neural Network from scratch</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="neural-networks.html"><a href="neural-networks.html#introduction-to-keras"><i class="fa fa-check"></i><b>3.2</b> Introduction to Keras</a><ul>
<li class="chapter" data-level="3.2.1" data-path="neural-networks.html"><a href="neural-networks.html#installing-keras"><i class="fa fa-check"></i><b>3.2.1</b> Installing keras</a></li>
<li class="chapter" data-level="3.2.2" data-path="neural-networks.html"><a href="neural-networks.html#building-model-with-keras"><i class="fa fa-check"></i><b>3.2.2</b> building model with keras</a></li>
<li class="chapter" data-level="3.2.3" data-path="neural-networks.html"><a href="neural-networks.html#the-kears-functional-api"><i class="fa fa-check"></i><b>3.2.3</b> The Kears Functional API</a></li>
<li class="chapter" data-level="3.2.4" data-path="neural-networks.html"><a href="neural-networks.html#models-as-directed-acyclic-graphs-of-layers"><i class="fa fa-check"></i><b>3.2.4</b> Models as Directed acyclic graphs of layers</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="neural-networks.html"><a href="neural-networks.html#monitoring-deep-learning-models"><i class="fa fa-check"></i><b>3.3</b> Monitoring deep learning models</a><ul>
<li class="chapter" data-level="3.3.1" data-path="neural-networks.html"><a href="neural-networks.html#using-callbacks"><i class="fa fa-check"></i><b>3.3.1</b> Using callbacks</a></li>
<li class="chapter" data-level="3.3.2" data-path="neural-networks.html"><a href="neural-networks.html#tensorboard"><i class="fa fa-check"></i><b>3.3.2</b> TensorBoard</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="neural-networks.html"><a href="neural-networks.html#batch-normalization"><i class="fa fa-check"></i><b>3.4</b> Batch normalization</a></li>
<li class="chapter" data-level="3.5" data-path="neural-networks.html"><a href="neural-networks.html#overfitting-handling"><i class="fa fa-check"></i><b>3.5</b> Overfitting handling</a><ul>
<li class="chapter" data-level="3.5.1" data-path="neural-networks.html"><a href="neural-networks.html#reducing-the-networks-size"><i class="fa fa-check"></i><b>3.5.1</b> Reducing the network’s size</a></li>
<li class="chapter" data-level="3.5.2" data-path="neural-networks.html"><a href="neural-networks.html#adding-weight-regularization"><i class="fa fa-check"></i><b>3.5.2</b> Adding weight regularization</a></li>
<li class="chapter" data-level="3.5.3" data-path="neural-networks.html"><a href="neural-networks.html#adding-dropout"><i class="fa fa-check"></i><b>3.5.3</b> Adding dropout</a></li>
<li class="chapter" data-level="3.5.4" data-path="neural-networks.html"><a href="neural-networks.html#data-augmentation"><i class="fa fa-check"></i><b>3.5.4</b> Data augmentation</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="neural-networks.html"><a href="neural-networks.html#hyperparameters-optimization"><i class="fa fa-check"></i><b>3.6</b> Hyperparameters optimization</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html"><i class="fa fa-check"></i><b>4</b> Deep learning for computer vision</a><ul>
<li class="chapter" data-level="4.1" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#image-classification-with-keras"><i class="fa fa-check"></i><b>4.1</b> Image classification with Keras</a><ul>
<li class="chapter" data-level="4.1.1" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#download-and-prepare-the-data"><i class="fa fa-check"></i><b>4.1.1</b> download and prepare the data</a></li>
<li class="chapter" data-level="4.1.2" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#build-the-model"><i class="fa fa-check"></i><b>4.1.2</b> Build the model</a></li>
<li class="chapter" data-level="4.1.3" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#compile-the-model"><i class="fa fa-check"></i><b>4.1.3</b> Compile the model</a></li>
<li class="chapter" data-level="4.1.4" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#fit-the-model"><i class="fa fa-check"></i><b>4.1.4</b> Fit the model</a></li>
<li class="chapter" data-level="4.1.5" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#make-predictions"><i class="fa fa-check"></i><b>4.1.5</b> Make predictions</a></li>
<li class="chapter" data-level="4.1.6" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#evaluate-the-model"><i class="fa fa-check"></i><b>4.1.6</b> Evaluate the model</a></li>
<li class="chapter" data-level="4.1.7" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#save-the-model"><i class="fa fa-check"></i><b>4.1.7</b> Save the model</a></li>
<li class="chapter" data-level="4.1.8" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#reload-the-model"><i class="fa fa-check"></i><b>4.1.8</b> reload the model</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#introduction-to-convolution-neural-networks"><i class="fa fa-check"></i><b>4.2</b> Introduction to Convolution Neural Networks</a><ul>
<li class="chapter" data-level="4.2.1" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#example"><i class="fa fa-check"></i><b>4.2.1</b> Example</a></li>
<li class="chapter" data-level="4.2.2" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#the-convolution-operation"><i class="fa fa-check"></i><b>4.2.2</b> The convolution operation</a></li>
<li class="chapter" data-level="4.2.3" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#the-max-pooling-operation"><i class="fa fa-check"></i><b>4.2.3</b> The max-pooling operation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#architectures-of-cnn"><i class="fa fa-check"></i><b>4.3</b> Architectures of CNN</a></li>
<li class="chapter" data-level="4.4" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#classifcation-examples"><i class="fa fa-check"></i><b>4.4</b> Classifcation examples</a><ul>
<li class="chapter" data-level="4.4.1" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#dataset-dog-vs-cats"><i class="fa fa-check"></i><b>4.4.1</b> Dataset: Dog VS Cats</a></li>
<li class="chapter" data-level="4.4.2" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#dataset-cifar10"><i class="fa fa-check"></i><b>4.4.2</b> Dataset: CIFAR10</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="transfer-learning.html"><a href="transfer-learning.html"><i class="fa fa-check"></i><b>5</b> Transfer learning</a><ul>
<li class="chapter" data-level="5.1" data-path="transfer-learning.html"><a href="transfer-learning.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="transfer-learning.html"><a href="transfer-learning.html#what-is-transfer-learning"><i class="fa fa-check"></i><b>5.1.1</b> What is Transfer Learning</a></li>
<li class="chapter" data-level="5.1.2" data-path="transfer-learning.html"><a href="transfer-learning.html#what-is-tensforflow-hub"><i class="fa fa-check"></i><b>5.1.2</b> What is TensforFlow Hub</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="transfer-learning.html"><a href="transfer-learning.html#transfer-learning-with-tensforflow-hub"><i class="fa fa-check"></i><b>5.2</b> Transfer learning with TensforFlow Hub</a><ul>
<li class="chapter" data-level="5.2.1" data-path="transfer-learning.html"><a href="transfer-learning.html#imagenet-classifier"><i class="fa fa-check"></i><b>5.2.1</b> ImageNet classifier</a></li>
<li class="chapter" data-level="5.2.2" data-path="transfer-learning.html"><a href="transfer-learning.html#transfer-learning-1"><i class="fa fa-check"></i><b>5.2.2</b> Transfer learning</a></li>
<li class="chapter" data-level="5.2.3" data-path="transfer-learning.html"><a href="transfer-learning.html#run-the-classifier-on-a-batch-of-images"><i class="fa fa-check"></i><b>5.2.3</b> Run the classifier on a batch of images</a></li>
<li class="chapter" data-level="5.2.4" data-path="transfer-learning.html"><a href="transfer-learning.html#download-a-headless-model"><i class="fa fa-check"></i><b>5.2.4</b> Download a headless model</a></li>
<li class="chapter" data-level="5.2.5" data-path="transfer-learning.html"><a href="transfer-learning.html#attach-a-classification-head"><i class="fa fa-check"></i><b>5.2.5</b> Attach a classification head</a></li>
<li class="chapter" data-level="5.2.6" data-path="transfer-learning.html"><a href="transfer-learning.html#train-the-model"><i class="fa fa-check"></i><b>5.2.6</b> Train the model</a></li>
<li class="chapter" data-level="5.2.7" data-path="transfer-learning.html"><a href="transfer-learning.html#export-the-model"><i class="fa fa-check"></i><b>5.2.7</b> Export the model</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="transfer-learning.html"><a href="transfer-learning.html#transfer-learning-using-a-pretrained-convnet"><i class="fa fa-check"></i><b>5.3</b> Transfer learning using a pretrained CONVNET</a><ul>
<li class="chapter" data-level="5.3.1" data-path="transfer-learning.html"><a href="transfer-learning.html#feature-extraction"><i class="fa fa-check"></i><b>5.3.1</b> feature extraction</a></li>
<li class="chapter" data-level="5.3.2" data-path="transfer-learning.html"><a href="transfer-learning.html#fine-tuning"><i class="fa fa-check"></i><b>5.3.2</b> Fine-tuning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="visualizing-what-convnets-leran.html"><a href="visualizing-what-convnets-leran.html"><i class="fa fa-check"></i><b>6</b> Visualizing what CONVNETS leran</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computer vision with R and keras</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="transfer-learning" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Transfer learning</h1>
<p><a href="https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751" class="uri">https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751</a>
<a href="https://keras.io/applications/#available-models" class="uri">https://keras.io/applications/#available-models</a></p>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">5.1</span> Introduction</h2>
<div id="what-is-transfer-learning" class="section level3">
<h3><span class="header-section-number">5.1.1</span> What is Transfer Learning</h3>
<p>Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task. In transfer learning, we first train a base network on a base dataset and task, and then we repurpose the learned features, or transfer them, to a second target network to be trained on a target dataset and task. This process will tend to work if the features are general, meaning suitable to both base and target tasks, instead of specific to the base task.</p>
</div>
<div id="what-is-tensforflow-hub" class="section level3">
<h3><span class="header-section-number">5.1.2</span> What is TensforFlow Hub</h3>
<p>TensorFlow Hub is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning. Transfer learning can:</p>
<ul>
<li>Train a model with a smaller dataset,</li>
<li>Improve generalization, and</li>
<li>Speed up training.</li>
</ul>
</div>
</div>
<div id="transfer-learning-with-tensforflow-hub" class="section level2">
<h2><span class="header-section-number">5.2</span> Transfer learning with TensforFlow Hub</h2>
<p>We load necessary packages</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb163-1" title="1"><span class="kw">library</span>(keras)</a>
<a class="sourceLine" id="cb163-2" title="2"><span class="kw">library</span>(tfhub)</a>
<a class="sourceLine" id="cb163-3" title="3"><span class="kw">library</span>(pins)</a></code></pre></div>
<div id="imagenet-classifier" class="section level3">
<h3><span class="header-section-number">5.2.1</span> ImageNet classifier</h3>
<div id="download-the-classifier" class="section level4">
<h4><span class="header-section-number">5.2.1.1</span> Download the classifier</h4>
<p>we can use <code>layer_hub</code> to load a mobilenet and wrap it as a keras layer. We have to select one classifier URL from (tfhub.dev)[<a href="https://tfhub.dev/" class="uri">https://tfhub.dev/</a>].</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb164-1" title="1"><span class="co"># select the classifier URL</span></a>
<a class="sourceLine" id="cb164-2" title="2">classifier_url =<span class="st">&quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2&quot;</span></a>
<a class="sourceLine" id="cb164-3" title="3"><span class="co"># define the image shape</span></a>
<a class="sourceLine" id="cb164-4" title="4">image_shape &lt;-<span class="st"> </span><span class="kw">c</span>(224L, 224L, 3L)</a>
<a class="sourceLine" id="cb164-5" title="5"><span class="co"># load the classifier</span></a>
<a class="sourceLine" id="cb164-6" title="6">classifier &lt;-<span class="st"> </span><span class="kw">layer_hub</span>(<span class="dt">handle =</span> classifier_url, <span class="dt">input_shape =</span> image_shape)</a></code></pre></div>
</div>
<div id="run-the-classifier-on-one-image" class="section level4">
<h4><span class="header-section-number">5.2.1.2</span> Run the classifier on one image</h4>
<p>In order to verify the classifier, we can run it on a single image</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb165-1" title="1"><span class="co"># we download an image</span></a>
<a class="sourceLine" id="cb165-2" title="2">image_url &lt;-<span class="st"> &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg&quot;</span></a>
<a class="sourceLine" id="cb165-3" title="3"></a>
<a class="sourceLine" id="cb165-4" title="4">img &lt;-<span class="st"> </span>pins<span class="op">::</span><span class="kw">pin</span>(image_url, <span class="dt">name =</span> <span class="st">&quot;grace_hopper&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb165-5" title="5"><span class="st">  </span>tensorflow<span class="op">::</span>tf<span class="op">$</span>io<span class="op">$</span><span class="kw">read_file</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb165-6" title="6"><span class="st">  </span>tensorflow<span class="op">::</span>tf<span class="op">$</span>image<span class="op">$</span><span class="kw">decode_image</span>(<span class="dt">dtype =</span> tf<span class="op">$</span>float32) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb165-7" title="7"><span class="st">  </span>tensorflow<span class="op">::</span>tf<span class="op">$</span>image<span class="op">$</span><span class="kw">resize</span>(<span class="dt">size =</span> image_shape[<span class="op">-</span><span class="dv">3</span>])</a></code></pre></div>
<p>we can plot the image</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb166-1" title="1">img <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb166-2" title="2"><span class="st">  </span><span class="kw">as.array</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb166-3" title="3"><span class="st">  </span><span class="kw">as.raster</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb166-4" title="4"><span class="st">  </span><span class="kw">plot</span>()</a></code></pre></div>
<p><img src="DeepLearning-ComputerVision_files/figure-html/unnamed-chunk-74-1.png" width="672" /></p>
<p>We add a batch dimension and pass the image to the model.</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb167-1" title="1">result &lt;-<span class="st"> </span>img <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb167-2" title="2"><span class="st">  </span>tf<span class="op">$</span><span class="kw">expand_dims</span>(0L) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb167-3" title="3"><span class="st">  </span><span class="kw">classifier</span>()</a>
<a class="sourceLine" id="cb167-4" title="4"><span class="kw">print</span>(result)</a></code></pre></div>
<pre><code>## tf.Tensor(
## [[ 0.18970814  1.2320935   0.00924352 ...  0.46654856 -0.43308693
##    0.07141374]], shape=(1, 1001), dtype=float32)</code></pre>
<p>The result consists on a 1001 element vector of logits, rating the probability of each class for the ilages. We use argmax in order to find the top class ID.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb169-1" title="1">predicted_class &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">argmax</span>(result, <span class="dt">axis =</span> 1L) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.integer</span>()</a>
<a class="sourceLine" id="cb169-2" title="2">predicted_class</a></code></pre></div>
<pre><code>## [1] 653</code></pre>
</div>
<div id="decode-the-prediction" class="section level4">
<h4><span class="header-section-number">5.2.1.3</span> Decode the prediction</h4>
<p>The classifier predicted that our image belong to the class of ID: 653. We need to identify the label corresponding to this class in the ImageNet data.</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb171-1" title="1"><span class="co"># we download the labels</span></a>
<a class="sourceLine" id="cb171-2" title="2">labels_url &lt;-<span class="st"> &quot;https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt&quot;</span></a>
<a class="sourceLine" id="cb171-3" title="3"></a>
<a class="sourceLine" id="cb171-4" title="4">imagenet_labels &lt;-<span class="st"> </span>pins<span class="op">::</span><span class="kw">pin</span>(labels_url, <span class="st">&quot;imagenet_labels&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb171-5" title="5"><span class="st">  </span><span class="kw">readLines</span>()</a></code></pre></div>
<p>We will plot the image with the label corresponding to teh predicted class</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb172-1" title="1">img <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb172-2" title="2"><span class="st">  </span><span class="kw">as.array</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb172-3" title="3"><span class="st">  </span><span class="kw">as.raster</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb172-4" title="4"><span class="st">  </span><span class="kw">plot</span>()</a>
<a class="sourceLine" id="cb172-5" title="5"><span class="co"># </span></a>
<a class="sourceLine" id="cb172-6" title="6"><span class="kw">title</span>(<span class="kw">paste</span>(<span class="st">&quot;Prediction:&quot;</span> , imagenet_labels[predicted_class <span class="op">+</span><span class="st"> </span><span class="dv">1</span>]))</a></code></pre></div>
<p><img src="DeepLearning-ComputerVision_files/figure-html/unnamed-chunk-78-1.png" width="672" /></p>
</div>
</div>
<div id="transfer-learning-1" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Transfer learning</h3>
<p>With the use of TF Hub we can retrain the top layer of the model to recognize the classes in our dataset</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb173-1" title="1">flowers &lt;-<span class="st"> </span>pins<span class="op">::</span><span class="kw">pin</span>(<span class="st">&quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;</span>, <span class="st">&quot;flower_photos&quot;</span>)</a></code></pre></div>
<p>We can use <code>image_data_generator</code> to load this data into our model.
Since TensorFlow Hub’s image modules used float inputs that range between 0 and 1, we have to rescale our images using <code>image_data_generator</code></p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb174-1" title="1">image_generator &lt;-<span class="st"> </span><span class="kw">image_data_generator</span>(<span class="dt">rescale=</span><span class="dv">1</span><span class="op">/</span><span class="dv">255</span>)</a>
<a class="sourceLine" id="cb174-2" title="2">image_data &lt;-<span class="st"> </span>flowers[<span class="dv">1</span>] <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb174-3" title="3"><span class="st">  </span><span class="kw">dirname</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb174-4" title="4"><span class="st">  </span><span class="kw">dirname</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb174-5" title="5"><span class="st">  </span><span class="kw">flow_images_from_directory</span>(image_generator, <span class="dt">target_size =</span> image_shape[<span class="op">-</span><span class="dv">3</span>])</a></code></pre></div>
<p>The reulting object is an iterator that returns image_batch, label_batch pairs. We can iterate over it using the <code>iter_nex</code> function.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb175-1" title="1"><span class="kw">str</span>(reticulate<span class="op">::</span><span class="kw">iter_next</span>(image_data))</a></code></pre></div>
<pre><code>## List of 2
##  $ : num [1:32, 1:224, 1:224, 1:3] 0.0157 0.2118 0.1373 0 0.9608 ...
##  $ : num [1:32, 1:3] 1 0 0 1 0 0 1 0 1 0 ...</code></pre>
</div>
<div id="run-the-classifier-on-a-batch-of-images" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Run the classifier on a batch of images</h3>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb177-1" title="1">image_batch &lt;-<span class="st"> </span>reticulate<span class="op">::</span><span class="kw">iter_next</span>(image_data)</a>
<a class="sourceLine" id="cb177-2" title="2">predictions &lt;-<span class="st"> </span><span class="kw">classifier</span>(tf<span class="op">$</span><span class="kw">constant</span>(image_batch[[<span class="dv">1</span>]], tf<span class="op">$</span>float32))</a>
<a class="sourceLine" id="cb177-3" title="3">predicted_classnames &lt;-<span class="st"> </span>imagenet_labels[<span class="kw">as.integer</span>(tf<span class="op">$</span><span class="kw">argmax</span>(predictions, <span class="dt">axis =</span> 1L) <span class="op">+</span><span class="st"> </span>1L)]</a></code></pre></div>
<p>We can plot the predicted classes with the images in order to evaluate the classifier performance</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb178-1" title="1"><span class="kw">par</span>(<span class="dt">mfcol =</span> <span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">8</span>), <span class="dt">mar =</span> <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">4</span>), <span class="dt">oma =</span> <span class="kw">rep</span>(<span class="fl">0.2</span>, <span class="dv">4</span>))</a>
<a class="sourceLine" id="cb178-2" title="2">image_batch[[<span class="dv">1</span>]] <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb178-3" title="3"><span class="st">  </span>purrr<span class="op">::</span><span class="kw">array_tree</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb178-4" title="4"><span class="st">  </span>purrr<span class="op">::</span><span class="kw">set_names</span>(predicted_classnames) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb178-5" title="5"><span class="st">  </span>purrr<span class="op">::</span><span class="kw">map</span>(as.raster) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb178-6" title="6"><span class="st">  </span>purrr<span class="op">::</span><span class="kw">iwalk</span>(<span class="op">~</span>{<span class="kw">plot</span>(.x); <span class="kw">title</span>(.y)})</a></code></pre></div>
<p><img src="DeepLearning-ComputerVision_files/figure-html/unnamed-chunk-83-1.png" width="672" /></p>
</div>
<div id="download-a-headless-model" class="section level3">
<h3><span class="header-section-number">5.2.4</span> Download a headless model</h3>
<p>In TensorFlow Hub we can use models without the top classification layer.</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb179-1" title="1">feature_extractor_url &lt;-<span class="st"> &quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2&quot;</span></a></code></pre></div>
<p>We create a festure extractor</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb180-1" title="1">feature_extractor_layer &lt;-<span class="st"> </span><span class="kw">layer_hub</span>(<span class="dt">handle =</span> feature_extractor_url, </a>
<a class="sourceLine" id="cb180-2" title="2">                                     <span class="dt">input_shape =</span> image_shape)</a></code></pre></div>
<p>It returns a 1280 length vector for each image</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb181-1" title="1">feature_batch &lt;-<span class="st"> </span><span class="kw">feature_extractor_layer</span>(tf<span class="op">$</span><span class="kw">constant</span>(image_batch[[<span class="dv">1</span>]], tf<span class="op">$</span>float32))</a>
<a class="sourceLine" id="cb181-2" title="2">feature_batch</a></code></pre></div>
<pre><code>## tf.Tensor(
## [[0.0000000e+00 1.7376544e+00 4.6723327e-03 ... 2.1834779e-01
##   0.0000000e+00 4.9047664e-02]
##  [7.7801757e-02 2.2452796e+00 0.0000000e+00 ... 0.0000000e+00
##   0.0000000e+00 0.0000000e+00]
##  [0.0000000e+00 6.9158578e-01 0.0000000e+00 ... 5.0055329e-02
##   0.0000000e+00 5.1555908e-01]
##  ...
##  [4.9063761e-02 1.2936193e+00 1.4952968e-01 ... 7.8607094e-04
##   2.3129751e-01 2.9109207e-01]
##  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00
##   0.0000000e+00 1.2216148e-01]
##  [2.7850335e-02 8.8660502e-01 1.8906282e-02 ... 2.9068547e-01
##   0.0000000e+00 0.0000000e+00]], shape=(32, 1280), dtype=float32)</code></pre>
<p>Freeze the variables in the feature extractor layer, so that the training only modifies the new classifier layer.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb183-1" title="1"><span class="kw">freeze_weights</span>(feature_extractor_layer)</a></code></pre></div>
</div>
<div id="attach-a-classification-head" class="section level3">
<h3><span class="header-section-number">5.2.5</span> Attach a classification head</h3>
<p>Now let’s create a sequential model using the feature extraction layer and add a new classification layer.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb184-1" title="1">model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>(<span class="kw">list</span>(</a>
<a class="sourceLine" id="cb184-2" title="2">  feature_extractor_layer,</a>
<a class="sourceLine" id="cb184-3" title="3">  <span class="kw">layer_dense</span>(<span class="dt">units =</span> image_data<span class="op">$</span>num_classes, <span class="dt">activation=</span><span class="st">&#39;softmax&#39;</span>)</a>
<a class="sourceLine" id="cb184-4" title="4">))</a>
<a class="sourceLine" id="cb184-5" title="5"></a>
<a class="sourceLine" id="cb184-6" title="6"><span class="kw">summary</span>(model)</a></code></pre></div>
<pre><code>## Model: &quot;sequential_5&quot;
## ________________________________________________________________________________
## Layer (type)                        Output Shape                    Param #     
## ================================================================================
## keras_layer_1 (KerasLayer)          (None, 1280)                    2257984     
## ________________________________________________________________________________
## dense_14 (Dense)                    (None, 3)                       3843        
## ================================================================================
## Total params: 2,261,827
## Trainable params: 3,843
## Non-trainable params: 2,257,984
## ________________________________________________________________________________</code></pre>
</div>
<div id="train-the-model" class="section level3">
<h3><span class="header-section-number">5.2.6</span> Train the model</h3>
<p>Use compile to configure the training process:</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb186-1" title="1">model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(</a>
<a class="sourceLine" id="cb186-2" title="2">  <span class="dt">optimizer =</span> <span class="st">&quot;adam&quot;</span>,</a>
<a class="sourceLine" id="cb186-3" title="3">  <span class="dt">loss =</span> <span class="st">&quot;categorical_crossentropy&quot;</span>,</a>
<a class="sourceLine" id="cb186-4" title="4">  <span class="dt">metrics =</span> <span class="st">&quot;accuracy&quot;</span></a>
<a class="sourceLine" id="cb186-5" title="5">)</a></code></pre></div>
<p>Now we use the <code>fit</code> method to train the model</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb187-1" title="1">history &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit_generator</span>(</a>
<a class="sourceLine" id="cb187-2" title="2">  image_data, <span class="dt">epochs=</span><span class="dv">5</span>, </a>
<a class="sourceLine" id="cb187-3" title="3">  <span class="dt">steps_per_epoch =</span> image_data<span class="op">$</span>n <span class="op">/</span><span class="st"> </span>image_data<span class="op">$</span>batch_size,</a>
<a class="sourceLine" id="cb187-4" title="4">  <span class="dt">verbose =</span> <span class="dv">2</span></a>
<a class="sourceLine" id="cb187-5" title="5">)</a></code></pre></div>
<p>Now just after 2 iterations we can see that the model is maing progress in the classification performance.</p>
</div>
<div id="export-the-model" class="section level3">
<h3><span class="header-section-number">5.2.7</span> Export the model</h3>
<p>Now we can save our trained model</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb188-1" title="1"><span class="kw">save_model_tf</span>(model, <span class="st">&quot;mymodel/&quot;</span>, <span class="dt">include_optimizer =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>Now confirm that we can reload it, and it still gives the same results:</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb189-1" title="1">model_ &lt;-<span class="st"> </span><span class="kw">load_model_tf</span>(<span class="st">&quot;mymodel/&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb190-1" title="1">x &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">constant</span>(image_batch[[<span class="dv">1</span>]], tf<span class="op">$</span>float32)</a>
<a class="sourceLine" id="cb190-2" title="2"><span class="kw">all.equal</span>(</a>
<a class="sourceLine" id="cb190-3" title="3">  <span class="kw">as.matrix</span>(<span class="kw">model</span>(x)),</a>
<a class="sourceLine" id="cb190-4" title="4">  <span class="kw">as.matrix</span>(<span class="kw">model_</span>(x))</a>
<a class="sourceLine" id="cb190-5" title="5">)</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
</div>
<div id="transfer-learning-using-a-pretrained-convnet" class="section level2">
<h2><span class="header-section-number">5.3</span> Transfer learning using a pretrained CONVNET</h2>
<div id="feature-extraction" class="section level3">
<h3><span class="header-section-number">5.3.1</span> feature extraction</h3>
<p>Feature extraction consits of using the representations learned by a trained network to extract interesting features from new samples. These features are then used in anew classifier trained from scratch. For convnets models, fature extraction consists of taking the convolution base of a previously trained network, running the new data through it, and training a new classifier on top of the output.
We reuse the convolution base because it is likely to be more generic compared to the representations learned by the classifier that are largely depending on the set of classes used for training.
The level og generality of the representations extracted by specific convolution layers depends on the depth of the layer in the model. Layers that come earlier in the model extract local, highly generic feature maps (such as visual edges, colors, and textures), wheras layers that are higher up extract more abstract concepts (such as “cat ear” or “dog eye”). So our new dataset differs a lot from the dataset on which the original model was trained, it would be better to use only the first new layers of the model for feature extraction instead of using the hwole convolutional base.</p>
<p>In this example, we will use the VGG16 network, trained on ImageNet data.
There are sevral image-classificationtrained models on Imagenet data: Xception, Inception V3, ResNet50, VGG16, VGG19, MobileNet…</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb192-1" title="1">conv_base =<span class="st"> </span><span class="kw">application_vgg16</span>(</a>
<a class="sourceLine" id="cb192-2" title="2">  <span class="dt">weights =</span> <span class="st">&quot;imagenet&quot;</span>,</a>
<a class="sourceLine" id="cb192-3" title="3">  <span class="dt">include_top =</span> <span class="ot">FALSE</span>, <span class="co">#including or not the densly connected classifier on top of the network.</span></a>
<a class="sourceLine" id="cb192-4" title="4">  <span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">150</span>,<span class="dv">150</span>,<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb192-5" title="5">)</a>
<a class="sourceLine" id="cb192-6" title="6"></a>
<a class="sourceLine" id="cb192-7" title="7">conv_base</a></code></pre></div>
<pre><code>## Model
## Model: &quot;vgg16&quot;
## ________________________________________________________________________________
## Layer (type)                        Output Shape                    Param #     
## ================================================================================
## input_2 (InputLayer)                [(None, 150, 150, 3)]           0           
## ________________________________________________________________________________
## block1_conv1 (Conv2D)               (None, 150, 150, 64)            1792        
## ________________________________________________________________________________
## block1_conv2 (Conv2D)               (None, 150, 150, 64)            36928       
## ________________________________________________________________________________
## block1_pool (MaxPooling2D)          (None, 75, 75, 64)              0           
## ________________________________________________________________________________
## block2_conv1 (Conv2D)               (None, 75, 75, 128)             73856       
## ________________________________________________________________________________
## block2_conv2 (Conv2D)               (None, 75, 75, 128)             147584      
## ________________________________________________________________________________
## block2_pool (MaxPooling2D)          (None, 37, 37, 128)             0           
## ________________________________________________________________________________
## block3_conv1 (Conv2D)               (None, 37, 37, 256)             295168      
## ________________________________________________________________________________
## block3_conv2 (Conv2D)               (None, 37, 37, 256)             590080      
## ________________________________________________________________________________
## block3_conv3 (Conv2D)               (None, 37, 37, 256)             590080      
## ________________________________________________________________________________
## block3_pool (MaxPooling2D)          (None, 18, 18, 256)             0           
## ________________________________________________________________________________
## block4_conv1 (Conv2D)               (None, 18, 18, 512)             1180160     
## ________________________________________________________________________________
## block4_conv2 (Conv2D)               (None, 18, 18, 512)             2359808     
## ________________________________________________________________________________
## block4_conv3 (Conv2D)               (None, 18, 18, 512)             2359808     
## ________________________________________________________________________________
## block4_pool (MaxPooling2D)          (None, 9, 9, 512)               0           
## ________________________________________________________________________________
## block5_conv1 (Conv2D)               (None, 9, 9, 512)               2359808     
## ________________________________________________________________________________
## block5_conv2 (Conv2D)               (None, 9, 9, 512)               2359808     
## ________________________________________________________________________________
## block5_conv3 (Conv2D)               (None, 9, 9, 512)               2359808     
## ________________________________________________________________________________
## block5_pool (MaxPooling2D)          (None, 4, 4, 512)               0           
## ================================================================================
## Total params: 14,714,688
## Trainable params: 14,714,688
## Non-trainable params: 0
## ________________________________________________________________________________</code></pre>
<p>There are two main methods to use to convolutional base network in our data:</p>
<ul>
<li>Running the convolutional base over our dataset, recording its output to an array on disk, and then using this data as input to a densly connected classifier. This method is fast and cheap to run, because it only requires running the convolutional base once for evey input image which is not the most expensive part of our model.</li>
<li>Adding dense layers on top of the convolutional base and running the whole model on the input data. This solution is expensive to run since we need to ru the whole model.</li>
</ul>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb194-1" title="1">model =<span class="st"> </span><span class="kw">keras_model_sequential</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb194-2" title="2"><span class="st">  </span>conv_base <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb194-3" title="3"><span class="st">  </span><span class="kw">layer_flatten</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb194-4" title="4"><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">256</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb194-5" title="5"><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">1</span>, <span class="dt">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</a></code></pre></div>
<p>Before compiling the model, we need to freeze the convolutional base: preventing the layers’ weights from being updated during the training.</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb195-1" title="1"><span class="kw">freeze_weights</span>(conv_base)</a></code></pre></div>
</div>
<div id="fine-tuning" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Fine-tuning</h3>
<p>Fine tuning is a complementary technique to feature extraction wehen we reuse a pretrained model. It consists of unfreezing a few of the top layers of a froze model base used for feature extraction, and jointly training both the newly added part of the model and these top layers.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="deep-learning-for-computer-vision.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="visualizing-what-convnets-leran.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DeepLearning-ComputerVision.pdf", "DeepLearning-ComputerVision.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
