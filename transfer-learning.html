<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Transfer learning | Computer vision with R and keras</title>
  <meta name="description" content="This book conains tutorials for deep learning applications in computer vision" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Transfer learning | Computer vision with R and keras" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book conains tutorials for deep learning applications in computer vision" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Transfer learning | Computer vision with R and keras" />
  
  <meta name="twitter:description" content="This book conains tutorials for deep learning applications in computer vision" />
  

<meta name="author" content="Saif Shabou" />


<meta name="date" content="2020-04-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="deep-learning-for-computer-vision.html"/>
<link rel="next" href="visualizing-what-convnets-leran.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Presentation</a></li>
<li class="chapter" data-level="2" data-path="foundamental-of-machine-learning.html"><a href="foundamental-of-machine-learning.html"><i class="fa fa-check"></i><b>2</b> Foundamental of machine learning</a><ul>
<li class="chapter" data-level="2.1" data-path="foundamental-of-machine-learning.html"><a href="foundamental-of-machine-learning.html#types"><i class="fa fa-check"></i><b>2.1</b> Types</a></li>
<li class="chapter" data-level="2.2" data-path="foundamental-of-machine-learning.html"><a href="foundamental-of-machine-learning.html#evaluation-of-ml-models"><i class="fa fa-check"></i><b>2.2</b> Evaluation of ML models</a><ul>
<li class="chapter" data-level="2.2.1" data-path="foundamental-of-machine-learning.html"><a href="foundamental-of-machine-learning.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>2.2.1</b> Training, validation and test sets</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="foundamental-of-machine-learning.html"><a href="foundamental-of-machine-learning.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>2.3</b> Overfitting and underfitting</a><ul>
<li class="chapter" data-level="2.3.1" data-path="foundamental-of-machine-learning.html"><a href="foundamental-of-machine-learning.html#reducing-the-networks-size"><i class="fa fa-check"></i><b>2.3.1</b> Reducing the network’s size</a></li>
<li class="chapter" data-level="2.3.2" data-path="foundamental-of-machine-learning.html"><a href="foundamental-of-machine-learning.html#adding-weight-regularization"><i class="fa fa-check"></i><b>2.3.2</b> Adding weight regularization</a></li>
<li class="chapter" data-level="2.3.3" data-path="foundamental-of-machine-learning.html"><a href="foundamental-of-machine-learning.html#adding-dropout"><i class="fa fa-check"></i><b>2.3.3</b> Adding dropout</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>3</b> Neural Networks</a><ul>
<li class="chapter" data-level="3.1" data-path="neural-networks.html"><a href="neural-networks.html#structure-of-neural-network"><i class="fa fa-check"></i><b>3.1</b> Structure of neural network</a><ul>
<li class="chapter" data-level="3.1.1" data-path="neural-networks.html"><a href="neural-networks.html#tensors"><i class="fa fa-check"></i><b>3.1.1</b> Tensors</a></li>
<li class="chapter" data-level="3.1.2" data-path="neural-networks.html"><a href="neural-networks.html#layers"><i class="fa fa-check"></i><b>3.1.2</b> Layers</a></li>
<li class="chapter" data-level="3.1.3" data-path="neural-networks.html"><a href="neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>3.1.3</b> Activation functions</a></li>
<li class="chapter" data-level="3.1.4" data-path="neural-networks.html"><a href="neural-networks.html#loss-functions-and-optimizers"><i class="fa fa-check"></i><b>3.1.4</b> Loss functions and optimizers</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="neural-networks.html"><a href="neural-networks.html#building-a-neural-network-from-scratch"><i class="fa fa-check"></i><b>3.2</b> Building a Neural Network from scratch</a></li>
<li class="chapter" data-level="3.3" data-path="neural-networks.html"><a href="neural-networks.html#introduction-to-keras"><i class="fa fa-check"></i><b>3.3</b> Introduction to Keras</a><ul>
<li class="chapter" data-level="3.3.1" data-path="neural-networks.html"><a href="neural-networks.html#installing-keras"><i class="fa fa-check"></i><b>3.3.1</b> Installing keras</a></li>
<li class="chapter" data-level="3.3.2" data-path="neural-networks.html"><a href="neural-networks.html#building-model-with-keras"><i class="fa fa-check"></i><b>3.3.2</b> building model with keras</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="neural-networks.html"><a href="neural-networks.html#classifying-movie-reviews-a-binary-classification-example"><i class="fa fa-check"></i><b>3.4</b> Classifying movie reviews: a binary classification example</a><ul>
<li class="chapter" data-level="3.4.1" data-path="neural-networks.html"><a href="neural-networks.html#the-imdb-dataset"><i class="fa fa-check"></i><b>3.4.1</b> The IMDB dataset</a></li>
<li class="chapter" data-level="3.4.2" data-path="neural-networks.html"><a href="neural-networks.html#preparing-the-data"><i class="fa fa-check"></i><b>3.4.2</b> preparing the data</a></li>
<li class="chapter" data-level="3.4.3" data-path="neural-networks.html"><a href="neural-networks.html#build-the-network"><i class="fa fa-check"></i><b>3.4.3</b> Build the network</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="neural-networks.html"><a href="neural-networks.html#the-kears-functional-api"><i class="fa fa-check"></i><b>3.5</b> The Kears Functional API</a><ul>
<li class="chapter" data-level="3.5.1" data-path="neural-networks.html"><a href="neural-networks.html#multi-input-models"><i class="fa fa-check"></i><b>3.5.1</b> Multi-input models</a></li>
<li class="chapter" data-level="3.5.2" data-path="neural-networks.html"><a href="neural-networks.html#multi-output-models"><i class="fa fa-check"></i><b>3.5.2</b> Multi-output models</a></li>
<li class="chapter" data-level="3.5.3" data-path="neural-networks.html"><a href="neural-networks.html#directed-acuclic-graphs-of-layers"><i class="fa fa-check"></i><b>3.5.3</b> Directed acuclic graphs of layers</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="neural-networks.html"><a href="neural-networks.html#monitoring-deep-learning-models"><i class="fa fa-check"></i><b>3.6</b> Monitoring deep learning models</a><ul>
<li class="chapter" data-level="3.6.1" data-path="neural-networks.html"><a href="neural-networks.html#using-callbacks"><i class="fa fa-check"></i><b>3.6.1</b> Using callbacks</a></li>
<li class="chapter" data-level="3.6.2" data-path="neural-networks.html"><a href="neural-networks.html#tensorboard"><i class="fa fa-check"></i><b>3.6.2</b> TensorBoard</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="neural-networks.html"><a href="neural-networks.html#batch-normalization"><i class="fa fa-check"></i><b>3.7</b> Batch normalization</a></li>
<li class="chapter" data-level="3.8" data-path="neural-networks.html"><a href="neural-networks.html#hyperparameters-optimization"><i class="fa fa-check"></i><b>3.8</b> Hyperparameters optimization</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html"><i class="fa fa-check"></i><b>4</b> Deep learning for computer vision</a><ul>
<li class="chapter" data-level="4.1" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#loading-image-data"><i class="fa fa-check"></i><b>4.1</b> Loading image data</a></li>
<li class="chapter" data-level="4.2" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#image-classification-with-keras"><i class="fa fa-check"></i><b>4.2</b> Image classification with Keras</a><ul>
<li class="chapter" data-level="4.2.1" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#download-and-prepare-the-data"><i class="fa fa-check"></i><b>4.2.1</b> download and prepare the data</a></li>
<li class="chapter" data-level="4.2.2" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#build-the-model"><i class="fa fa-check"></i><b>4.2.2</b> Build the model</a></li>
<li class="chapter" data-level="4.2.3" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#compile-the-model"><i class="fa fa-check"></i><b>4.2.3</b> Compile the model</a></li>
<li class="chapter" data-level="4.2.4" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#fit-the-model"><i class="fa fa-check"></i><b>4.2.4</b> Fit the model</a></li>
<li class="chapter" data-level="4.2.5" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#make-predictions"><i class="fa fa-check"></i><b>4.2.5</b> Make predictions</a></li>
<li class="chapter" data-level="4.2.6" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#evaluate-the-model"><i class="fa fa-check"></i><b>4.2.6</b> Evaluate the model</a></li>
<li class="chapter" data-level="4.2.7" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#save-the-model"><i class="fa fa-check"></i><b>4.2.7</b> Save the model</a></li>
<li class="chapter" data-level="4.2.8" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#reload-the-model"><i class="fa fa-check"></i><b>4.2.8</b> reload the model</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#introduction-to-convolution-neural-networks"><i class="fa fa-check"></i><b>4.3</b> Introduction to COnvolution Neural Networks</a><ul>
<li class="chapter" data-level="4.3.1" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#example"><i class="fa fa-check"></i><b>4.3.1</b> Example</a></li>
<li class="chapter" data-level="4.3.2" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#the-convolution-operation"><i class="fa fa-check"></i><b>4.3.2</b> The convolution operation</a></li>
<li class="chapter" data-level="4.3.3" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#the-max-pooling-operation"><i class="fa fa-check"></i><b>4.3.3</b> The max-pooling operation</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#architectures-of-cnn"><i class="fa fa-check"></i><b>4.4</b> Architectures of CNN</a></li>
<li class="chapter" data-level="4.5" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#application-dog-vs-cats"><i class="fa fa-check"></i><b>4.5</b> Application Dog VS Cats</a><ul>
<li class="chapter" data-level="4.5.1" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#downloading-data"><i class="fa fa-check"></i><b>4.5.1</b> Downloading data</a></li>
<li class="chapter" data-level="4.5.2" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#building-the-network"><i class="fa fa-check"></i><b>4.5.2</b> Building the network</a></li>
<li class="chapter" data-level="4.5.3" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#data-preprocessing"><i class="fa fa-check"></i><b>4.5.3</b> Data preprocessing</a></li>
<li class="chapter" data-level="4.5.4" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#data-augmentation"><i class="fa fa-check"></i><b>4.5.4</b> Data augmentation</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#cnn-with-cifar10-dataset"><i class="fa fa-check"></i><b>4.6</b> CNN with CIFAR10 dataset</a><ul>
<li class="chapter" data-level="4.6.1" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#download-and-prepare-the-cifar10-dataset"><i class="fa fa-check"></i><b>4.6.1</b> Download and prepare the CIFAR10 dataset</a></li>
<li class="chapter" data-level="4.6.2" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#build-the-model-1"><i class="fa fa-check"></i><b>4.6.2</b> Build the model</a></li>
<li class="chapter" data-level="4.6.3" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#compile-and-train-the-model"><i class="fa fa-check"></i><b>4.6.3</b> Compile and train the model</a></li>
<li class="chapter" data-level="4.6.4" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#evaluate-the-model-1"><i class="fa fa-check"></i><b>4.6.4</b> Evaluate the model</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="deep-learning-for-computer-vision.html"><a href="deep-learning-for-computer-vision.html#cnn-with-fruits-images"><i class="fa fa-check"></i><b>4.7</b> CNN with fruits images</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="transfer-learning.html"><a href="transfer-learning.html"><i class="fa fa-check"></i><b>5</b> Transfer learning</a><ul>
<li class="chapter" data-level="5.1" data-path="transfer-learning.html"><a href="transfer-learning.html#knowledge"><i class="fa fa-check"></i><b>5.1</b> Knowledge</a><ul>
<li class="chapter" data-level="5.1.1" data-path="transfer-learning.html"><a href="transfer-learning.html#what-is-transfer-learning"><i class="fa fa-check"></i><b>5.1.1</b> What is Transfer Learning</a></li>
<li class="chapter" data-level="5.1.2" data-path="transfer-learning.html"><a href="transfer-learning.html#what-is-tensforflow-hub"><i class="fa fa-check"></i><b>5.1.2</b> What is TensforFlow Hub</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="transfer-learning.html"><a href="transfer-learning.html#transfer-learning-with-tensforflow-hub"><i class="fa fa-check"></i><b>5.2</b> Transfer learning with TensforFlow Hub</a><ul>
<li class="chapter" data-level="5.2.1" data-path="transfer-learning.html"><a href="transfer-learning.html#imagenet-classifier"><i class="fa fa-check"></i><b>5.2.1</b> ImageNet classifier</a></li>
<li class="chapter" data-level="5.2.2" data-path="transfer-learning.html"><a href="transfer-learning.html#transfer-learning-1"><i class="fa fa-check"></i><b>5.2.2</b> Transfer learning</a></li>
<li class="chapter" data-level="5.2.3" data-path="transfer-learning.html"><a href="transfer-learning.html#run-the-classifier-on-a-batch-of-images"><i class="fa fa-check"></i><b>5.2.3</b> Run the classifier on a batch of images</a></li>
<li class="chapter" data-level="5.2.4" data-path="transfer-learning.html"><a href="transfer-learning.html#download-a-headless-model"><i class="fa fa-check"></i><b>5.2.4</b> Download a headless model</a></li>
<li class="chapter" data-level="5.2.5" data-path="transfer-learning.html"><a href="transfer-learning.html#attach-a-classification-head"><i class="fa fa-check"></i><b>5.2.5</b> Attach a classification head</a></li>
<li class="chapter" data-level="5.2.6" data-path="transfer-learning.html"><a href="transfer-learning.html#train-the-model"><i class="fa fa-check"></i><b>5.2.6</b> Train the model</a></li>
<li class="chapter" data-level="5.2.7" data-path="transfer-learning.html"><a href="transfer-learning.html#export-the-model"><i class="fa fa-check"></i><b>5.2.7</b> Export the model</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="transfer-learning.html"><a href="transfer-learning.html#transfer-learningusing-a-pretrained-convnet"><i class="fa fa-check"></i><b>5.3</b> Transfer learningusing a pretrained CONVNET</a><ul>
<li class="chapter" data-level="5.3.1" data-path="transfer-learning.html"><a href="transfer-learning.html#feature-extraction"><i class="fa fa-check"></i><b>5.3.1</b> feature extraction</a></li>
<li class="chapter" data-level="5.3.2" data-path="transfer-learning.html"><a href="transfer-learning.html#fine-tuning"><i class="fa fa-check"></i><b>5.3.2</b> Fine-tuning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="visualizing-what-convnets-leran.html"><a href="visualizing-what-convnets-leran.html"><i class="fa fa-check"></i><b>6</b> Visualizing what CONVNETS leran</a><ul>
<li class="chapter" data-level="6.1" data-path="visualizing-what-convnets-leran.html"><a href="visualizing-what-convnets-leran.html#visualizing-intermediate-activations"><i class="fa fa-check"></i><b>6.1</b> Visualizing intermediate activations</a></li>
<li class="chapter" data-level="6.2" data-path="visualizing-what-convnets-leran.html"><a href="visualizing-what-convnets-leran.html#visualizing-convnet-filters"><i class="fa fa-check"></i><b>6.2</b> Visualizing convnet filters</a></li>
<li class="chapter" data-level="6.3" data-path="visualizing-what-convnets-leran.html"><a href="visualizing-what-convnets-leran.html#visualizing-heatmaps-of-class-activation"><i class="fa fa-check"></i><b>6.3</b> Visualizing heatmaps of class activation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computer vision with R and keras</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="transfer-learning" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Transfer learning</h1>
<p><a href="https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751" class="uri">https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751</a>
<a href="https://keras.io/applications/#available-models" class="uri">https://keras.io/applications/#available-models</a></p>
<div id="knowledge" class="section level2">
<h2><span class="header-section-number">5.1</span> Knowledge</h2>
<div id="what-is-transfer-learning" class="section level3">
<h3><span class="header-section-number">5.1.1</span> What is Transfer Learning</h3>
<p>mobilenet</p>
</div>
<div id="what-is-tensforflow-hub" class="section level3">
<h3><span class="header-section-number">5.1.2</span> What is TensforFlow Hub</h3>
</div>
</div>
<div id="transfer-learning-with-tensforflow-hub" class="section level2">
<h2><span class="header-section-number">5.2</span> Transfer learning with TensforFlow Hub</h2>
<p>We load necessary packages</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" title="1"><span class="kw">library</span>(keras)</a>
<a class="sourceLine" id="cb88-2" title="2"><span class="kw">library</span>(tfhub)</a>
<a class="sourceLine" id="cb88-3" title="3"><span class="kw">library</span>(pins)</a></code></pre></div>
<div id="imagenet-classifier" class="section level3">
<h3><span class="header-section-number">5.2.1</span> ImageNet classifier</h3>
<div id="download-the-classifier" class="section level4">
<h4><span class="header-section-number">5.2.1.1</span> Download the classifier</h4>
<p>we can use <code>layer_hub</code> to load a mobilenet and wrap it as a keras layer. We have to select one classifier URL from (tfhub.dev)[<a href="https://tfhub.dev/" class="uri">https://tfhub.dev/</a>].</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb89-1" title="1"><span class="co"># select the classifier URL</span></a>
<a class="sourceLine" id="cb89-2" title="2">classifier_url =<span class="st">&quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2&quot;</span></a>
<a class="sourceLine" id="cb89-3" title="3"><span class="co"># define the image shape</span></a>
<a class="sourceLine" id="cb89-4" title="4">image_shape &lt;-<span class="st"> </span><span class="kw">c</span>(224L, 224L, 3L)</a>
<a class="sourceLine" id="cb89-5" title="5"><span class="co"># load the classifier</span></a>
<a class="sourceLine" id="cb89-6" title="6">classifier &lt;-<span class="st"> </span><span class="kw">layer_hub</span>(<span class="dt">handle =</span> classifier_url, <span class="dt">input_shape =</span> image_shape)</a></code></pre></div>
</div>
<div id="run-the-classifier-on-one-image" class="section level4">
<h4><span class="header-section-number">5.2.1.2</span> Run the classifier on one image</h4>
<p>In order to verify the classifier, we can run it on a single image</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" title="1"><span class="co"># we download an image</span></a>
<a class="sourceLine" id="cb90-2" title="2">image_url &lt;-<span class="st"> &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg&quot;</span></a>
<a class="sourceLine" id="cb90-3" title="3"></a>
<a class="sourceLine" id="cb90-4" title="4">img &lt;-<span class="st"> </span>pins<span class="op">::</span><span class="kw">pin</span>(image_url, <span class="dt">name =</span> <span class="st">&quot;grace_hopper&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb90-5" title="5"><span class="st">  </span>tensorflow<span class="op">::</span>tf<span class="op">$</span>io<span class="op">$</span><span class="kw">read_file</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb90-6" title="6"><span class="st">  </span>tensorflow<span class="op">::</span>tf<span class="op">$</span>image<span class="op">$</span><span class="kw">decode_image</span>(<span class="dt">dtype =</span> tf<span class="op">$</span>float32) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb90-7" title="7"><span class="st">  </span>tensorflow<span class="op">::</span>tf<span class="op">$</span>image<span class="op">$</span><span class="kw">resize</span>(<span class="dt">size =</span> image_shape[<span class="op">-</span><span class="dv">3</span>])</a></code></pre></div>
<p>we can plot the image</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" title="1">img <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb91-2" title="2"><span class="st">  </span><span class="kw">as.array</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb91-3" title="3"><span class="st">  </span><span class="kw">as.raster</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb91-4" title="4"><span class="st">  </span><span class="kw">plot</span>()</a></code></pre></div>
<p>We add a batch dimension and pass the image to the model.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" title="1">result &lt;-<span class="st"> </span>img <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb92-2" title="2"><span class="st">  </span>tf<span class="op">$</span><span class="kw">expand_dims</span>(0L) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb92-3" title="3"><span class="st">  </span><span class="kw">classifier</span>()</a>
<a class="sourceLine" id="cb92-4" title="4"><span class="kw">print</span>(result)</a></code></pre></div>
<p>The result consists on a 1001 element vector of logits, rating the probability of each class for the ilages. We use argmax in order to find the top class ID.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" title="1">predicted_class &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">argmax</span>(result, <span class="dt">axis =</span> 1L) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.integer</span>()</a>
<a class="sourceLine" id="cb93-2" title="2">predicted_class</a></code></pre></div>
</div>
<div id="decode-the-prediction" class="section level4">
<h4><span class="header-section-number">5.2.1.3</span> Decode the prediction</h4>
<p>The classifier predicted that our image belong to the class of ID: 653. We need to identify the label corresponding to this class in the ImageNet data.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" title="1"><span class="co"># we download the labels</span></a>
<a class="sourceLine" id="cb94-2" title="2">labels_url &lt;-<span class="st"> &quot;https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt&quot;</span></a>
<a class="sourceLine" id="cb94-3" title="3"></a>
<a class="sourceLine" id="cb94-4" title="4">imagenet_labels &lt;-<span class="st"> </span>pins<span class="op">::</span><span class="kw">pin</span>(labels_url, <span class="st">&quot;imagenet_labels&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb94-5" title="5"><span class="st">  </span><span class="kw">readLines</span>()</a></code></pre></div>
<p>We will plot the image with the label corresponding to teh predicted class</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb95-1" title="1">img <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb95-2" title="2"><span class="st">  </span><span class="kw">as.array</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb95-3" title="3"><span class="st">  </span><span class="kw">as.raster</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb95-4" title="4"><span class="st">  </span><span class="kw">plot</span>()</a>
<a class="sourceLine" id="cb95-5" title="5"><span class="co"># </span></a>
<a class="sourceLine" id="cb95-6" title="6"><span class="kw">title</span>(<span class="kw">paste</span>(<span class="st">&quot;Prediction:&quot;</span> , imagenet_labels[predicted_class <span class="op">+</span><span class="st"> </span><span class="dv">1</span>]))</a></code></pre></div>
</div>
</div>
<div id="transfer-learning-1" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Transfer learning</h3>
<p>With the use of TF Hub we can retrain the top layer of the model to recognize the classes in our dataset</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" title="1">flowers &lt;-<span class="st"> </span>pins<span class="op">::</span><span class="kw">pin</span>(<span class="st">&quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;</span>, <span class="st">&quot;flower_photos&quot;</span>)</a></code></pre></div>
<p>We can use <code>image_data_generator</code> to load this data into our model.
Since TensorFlow Hub’s image modules used float inputs that range between 0 and 1, we have to rescale our images using <code>image_data_generator</code></p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb97-1" title="1">image_generator &lt;-<span class="st"> </span><span class="kw">image_data_generator</span>(<span class="dt">rescale=</span><span class="dv">1</span><span class="op">/</span><span class="dv">255</span>)</a>
<a class="sourceLine" id="cb97-2" title="2">image_data &lt;-<span class="st"> </span>flowers[<span class="dv">1</span>] <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb97-3" title="3"><span class="st">  </span><span class="kw">dirname</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb97-4" title="4"><span class="st">  </span><span class="kw">dirname</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb97-5" title="5"><span class="st">  </span><span class="kw">flow_images_from_directory</span>(image_generator, <span class="dt">target_size =</span> image_shape[<span class="op">-</span><span class="dv">3</span>])</a></code></pre></div>
<p>The reulting object is an iterator that returns image_batch, label_batch pairs. We can iterate over it using the <code>iter_nex</code> function.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" title="1"><span class="kw">str</span>(reticulate<span class="op">::</span><span class="kw">iter_next</span>(image_data))</a></code></pre></div>
</div>
<div id="run-the-classifier-on-a-batch-of-images" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Run the classifier on a batch of images</h3>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" title="1">image_batch &lt;-<span class="st"> </span>reticulate<span class="op">::</span><span class="kw">iter_next</span>(image_data)</a>
<a class="sourceLine" id="cb99-2" title="2">predictions &lt;-<span class="st"> </span><span class="kw">classifier</span>(tf<span class="op">$</span><span class="kw">constant</span>(image_batch[[<span class="dv">1</span>]], tf<span class="op">$</span>float32))</a>
<a class="sourceLine" id="cb99-3" title="3">predicted_classnames &lt;-<span class="st"> </span>imagenet_labels[<span class="kw">as.integer</span>(tf<span class="op">$</span><span class="kw">argmax</span>(predictions, <span class="dt">axis =</span> 1L) <span class="op">+</span><span class="st"> </span>1L)]</a></code></pre></div>
<p>We can plot the predicted classes with the images in order to evaluate the classifier performance</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" title="1"><span class="kw">par</span>(<span class="dt">mfcol =</span> <span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">8</span>), <span class="dt">mar =</span> <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">4</span>), <span class="dt">oma =</span> <span class="kw">rep</span>(<span class="fl">0.2</span>, <span class="dv">4</span>))</a>
<a class="sourceLine" id="cb100-2" title="2">image_batch[[<span class="dv">1</span>]] <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb100-3" title="3"><span class="st">  </span>purrr<span class="op">::</span><span class="kw">array_tree</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb100-4" title="4"><span class="st">  </span>purrr<span class="op">::</span><span class="kw">set_names</span>(predicted_classnames) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb100-5" title="5"><span class="st">  </span>purrr<span class="op">::</span><span class="kw">map</span>(as.raster) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb100-6" title="6"><span class="st">  </span>purrr<span class="op">::</span><span class="kw">iwalk</span>(<span class="op">~</span>{<span class="kw">plot</span>(.x); <span class="kw">title</span>(.y)})</a></code></pre></div>
</div>
<div id="download-a-headless-model" class="section level3">
<h3><span class="header-section-number">5.2.4</span> Download a headless model</h3>
<p>In TensorFlow Hub we can use models without the top classification layer.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb101-1" title="1">feature_extractor_url &lt;-<span class="st"> &quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2&quot;</span></a></code></pre></div>
<p>We create a festure extractor</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" title="1">feature_extractor_layer &lt;-<span class="st"> </span><span class="kw">layer_hub</span>(<span class="dt">handle =</span> feature_extractor_url, </a>
<a class="sourceLine" id="cb102-2" title="2">                                     <span class="dt">input_shape =</span> image_shape)</a></code></pre></div>
<p>It returns a 1280 length vector for each image</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" title="1">feature_batch &lt;-<span class="st"> </span><span class="kw">feature_extractor_layer</span>(tf<span class="op">$</span><span class="kw">constant</span>(image_batch[[<span class="dv">1</span>]], tf<span class="op">$</span>float32))</a>
<a class="sourceLine" id="cb103-2" title="2">feature_batch</a></code></pre></div>
<p>Freeze the variables in the feature extractor layer, so that the training only modifies the new classifier layer.</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb104-1" title="1"><span class="kw">freeze_weights</span>(feature_extractor_layer)</a></code></pre></div>
</div>
<div id="attach-a-classification-head" class="section level3">
<h3><span class="header-section-number">5.2.5</span> Attach a classification head</h3>
<p>Now let’s create a sequential model using the feature extraction layer and add a new classification layer.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb105-1" title="1">model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>(<span class="kw">list</span>(</a>
<a class="sourceLine" id="cb105-2" title="2">  feature_extractor_layer,</a>
<a class="sourceLine" id="cb105-3" title="3">  <span class="kw">layer_dense</span>(<span class="dt">units =</span> image_data<span class="op">$</span>num_classes, <span class="dt">activation=</span><span class="st">&#39;softmax&#39;</span>)</a>
<a class="sourceLine" id="cb105-4" title="4">))</a>
<a class="sourceLine" id="cb105-5" title="5"></a>
<a class="sourceLine" id="cb105-6" title="6"><span class="kw">summary</span>(model)</a></code></pre></div>
</div>
<div id="train-the-model" class="section level3">
<h3><span class="header-section-number">5.2.6</span> Train the model</h3>
<p>Use compile to configure the training process:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb106-1" title="1">model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(</a>
<a class="sourceLine" id="cb106-2" title="2">  <span class="dt">optimizer =</span> <span class="st">&quot;adam&quot;</span>,</a>
<a class="sourceLine" id="cb106-3" title="3">  <span class="dt">loss =</span> <span class="st">&quot;categorical_crossentropy&quot;</span>,</a>
<a class="sourceLine" id="cb106-4" title="4">  <span class="dt">metrics =</span> <span class="st">&quot;accuracy&quot;</span></a>
<a class="sourceLine" id="cb106-5" title="5">)</a></code></pre></div>
<p>Now we use the <code>fit</code> method to train the model</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb107-1" title="1">history &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit_generator</span>(</a>
<a class="sourceLine" id="cb107-2" title="2">  image_data, <span class="dt">epochs=</span><span class="dv">2</span>, </a>
<a class="sourceLine" id="cb107-3" title="3">  <span class="dt">steps_per_epoch =</span> image_data<span class="op">$</span>n <span class="op">/</span><span class="st"> </span>image_data<span class="op">$</span>batch_size,</a>
<a class="sourceLine" id="cb107-4" title="4">  <span class="dt">verbose =</span> <span class="dv">2</span></a>
<a class="sourceLine" id="cb107-5" title="5">)</a></code></pre></div>
<p>Now just after 2 iterations we can see that the model is maing progress in the classification performance.</p>
</div>
<div id="export-the-model" class="section level3">
<h3><span class="header-section-number">5.2.7</span> Export the model</h3>
<p>Now we can save our trained model</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" title="1"><span class="kw">save_model_tf</span>(model, <span class="st">&quot;mymodel/&quot;</span>, <span class="dt">include_optimizer =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>Now confirm that we can reload it, and it still gives the same results:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb109-1" title="1">model_ &lt;-<span class="st"> </span><span class="kw">load_model_tf</span>(<span class="st">&quot;mymodel/&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb110-1" title="1">x &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">constant</span>(image_batch[[<span class="dv">1</span>]], tf<span class="op">$</span>float32)</a>
<a class="sourceLine" id="cb110-2" title="2"><span class="kw">all.equal</span>(</a>
<a class="sourceLine" id="cb110-3" title="3">  <span class="kw">as.matrix</span>(<span class="kw">model</span>(x)),</a>
<a class="sourceLine" id="cb110-4" title="4">  <span class="kw">as.matrix</span>(<span class="kw">model_</span>(x))</a>
<a class="sourceLine" id="cb110-5" title="5">)</a></code></pre></div>
</div>
</div>
<div id="transfer-learningusing-a-pretrained-convnet" class="section level2">
<h2><span class="header-section-number">5.3</span> Transfer learningusing a pretrained CONVNET</h2>
<div id="feature-extraction" class="section level3">
<h3><span class="header-section-number">5.3.1</span> feature extraction</h3>
<p>Feature extraction consits of using the representations learned by a trained network to extract interesting features from new samples. These features are then used in anew classifier trained from scratch. For convnets models, fature extraction consists of taking the convolution base of a previously trained network, running the new data through it, and training a new classifier on top of the output.
We reuse the convolution base because it is likely to be more generic compared to the representations learned by the classifier that are largely depending on the set of classes used for training.
The level og generality of the representations extracted by specific convolution layers depends on the depth of the layer in the model. Layers that come earlier in the model extract local, highly generic feature maps (such as visual edges, colors, and textures), wheras layers that are higher up extract more abstract concepts (such as “cat ear” or “dog eye”). So our new dataset differs a lot from the dataset on which the original model was trained, it would be better to use only the first new layers of the model for feature extraction instead of using the hwole convolutional base.</p>
<p>In this example, we will use the VGG16 network, trained on ImageNet data.
There are sevral image-classificationtrained models on Imagenet data: Xception, Inception V3, ResNet50, VGG16, VGG19, MobileNet…</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" title="1">conv_base =<span class="st"> </span><span class="kw">application_vgg16</span>(</a>
<a class="sourceLine" id="cb111-2" title="2">  <span class="dt">weights =</span> <span class="st">&quot;imagenet&quot;</span>,</a>
<a class="sourceLine" id="cb111-3" title="3">  <span class="dt">include_top =</span> <span class="ot">FALSE</span>, <span class="co">#including or not the densly connected classifier on top of the network.</span></a>
<a class="sourceLine" id="cb111-4" title="4">  <span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">150</span>,<span class="dv">150</span>,<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb111-5" title="5">)</a>
<a class="sourceLine" id="cb111-6" title="6"></a>
<a class="sourceLine" id="cb111-7" title="7">conv_base</a></code></pre></div>
<p>There are two main methods to use to convolutional base network in our data:</p>
<ul>
<li>Running the convolutional base over our dataset, recording its output to an array on disk, and then using this data as input to a densly connected classifier. This method is fast and cheap to run, because it only requires running the convolutional base once for evey input image which is not the most expensive part of our model.</li>
<li>Adding dense layers on top of the convolutional base and running the whole model on the input data. This solution is expensive to run since we need to ru the whole model.</li>
</ul>
<div id="fast-feature-extraction-without-data-augmentation" class="section level4">
<h4><span class="header-section-number">5.3.1.1</span> Fast feature extraction without data augmentation</h4>
</div>
<div id="feature-extraction-without-data-augmentation" class="section level4">
<h4><span class="header-section-number">5.3.1.2</span> feature extraction without data augmentation</h4>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb112-1" title="1">model =<span class="st"> </span><span class="kw">keras_model_sequential</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb112-2" title="2"><span class="st">  </span>conv_base <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb112-3" title="3"><span class="st">  </span><span class="kw">layer_flatten</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb112-4" title="4"><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">256</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb112-5" title="5"><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">1</span>, <span class="dt">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</a></code></pre></div>
<p>Before compiling the model, we need to freeze the convolutional base: preventing the layers’ weights from being updated during the training.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" title="1"><span class="kw">freeze_weights</span>(conv_base)</a></code></pre></div>
</div>
</div>
<div id="fine-tuning" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Fine-tuning</h3>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="deep-learning-for-computer-vision.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="visualizing-what-convnets-leran.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DeepLearning-ComputerVision.pdf", "DeepLearning-ComputerVision.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
